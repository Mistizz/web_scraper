#!/usr/bin/env python3
"""
NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«
æŒ‡å®šã•ã‚ŒãŸWebã‚µã‚¤ãƒˆã®å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã¾ã™ã€‚
JavaScriptå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œã€‚
"""

import argparse
import requests
import time
import re
import csv
import os
import json
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import BeautifulSoup
from typing import Set, List
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Seleniumé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_urls_from_file(file_path: str) -> List[str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰
        
    Returns:
        List[str]: URLã®ãƒªã‚¹ãƒˆ
    """
    urls = []
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    
    file_extension = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_extension == '.txt':
            # txtãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼š1è¡Œ1URL
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):  # ç©ºè¡Œã¨ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤å¤–
                        if line.startswith('http://') or line.startswith('https://'):
                            urls.append(line)
                        else:
                            logger.warning(f"ç„¡åŠ¹ãªURLï¼ˆè¡Œ{line_num}ï¼‰: {line}")
        
        elif file_extension == '.csv':
            # csvãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼š1åˆ—ç›®ãŒURLã€ã¾ãŸã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã§'url'åˆ—ã‚’æŒ‡å®š
            with open(file_path, 'r', encoding='utf-8') as f:
                # æœ€åˆã®è¡Œã‚’ç¢ºèªã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã©ã†ã‹åˆ¤å®š
                first_line = f.readline().strip()
                f.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«æˆ»ã‚‹
                
                reader = csv.reader(f)
                headers = next(reader)  # æœ€åˆã®è¡Œã‚’èª­ã‚€
                
                # 'url'åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                url_column_index = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1åˆ—ç›®
                if 'url' in [h.lower() for h in headers]:
                    url_column_index = [h.lower() for h in headers].index('url')
                elif not (first_line.startswith('http://') or first_line.startswith('https://')):
                    # æœ€åˆã®è¡ŒãŒURLã§ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ã—ã¦æ‰±ã†
                    pass  # url_column_indexã¯0ã®ã¾ã¾
                else:
                    # æœ€åˆã®è¡ŒãŒURLã®å ´åˆã¯ã€ãã‚Œã‚‚å‡¦ç†å¯¾è±¡ã«å«ã‚ã‚‹
                    f.seek(0)
                    reader = csv.reader(f)
                
                for row_num, row in enumerate(reader, 1):
                    if row and len(row) > url_column_index:
                        url = row[url_column_index].strip()
                        if url and (url.startswith('http://') or url.startswith('https://')):
                            urls.append(url)
                        elif url:
                            logger.warning(f"ç„¡åŠ¹ãªURLï¼ˆè¡Œ{row_num}ï¼‰: {url}")
        
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")
    
    except Exception as e:
        logger.error(f"URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    if not urls:
        raise ValueError("æœ‰åŠ¹ãªURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    logger.info(f"URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{len(urls)}å€‹ã®URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    return urls


class WebsiteScraper:
    def __init__(self, base_url: str, max_pages: int = 1000, delay: float = 1.0, 
                 base_path: str = None, pages_per_file: int = 80, use_javascript: bool = False):
        """
        Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆNoneã®å ´åˆã¯ç„¡åˆ¶é™ï¼‰
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
            base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆåˆ†å‰²å‡ºåŠ›ç”¨ï¼‰
            use_javascript: JavaScriptã‚’å®Ÿè¡Œã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.pages_per_file = pages_per_file
        self.use_javascript = use_javascript
        self.driver = None
        self.visited_urls: Set[str] = set()
        self.to_visit_urls: List[str] = [base_url]
        self.extracted_content: List[str] = []
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’å–å¾—
        parsed_url = urlparse(base_url)
        self.base_domain = parsed_url.netloc
        
        if base_path is not None:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå ´åˆ
            self.base_path = base_path
            if not self.base_path.startswith('/'):
                self.base_path = '/' + self.base_path
            if not self.base_path.endswith('/'):
                self.base_path = self.base_path + '/'
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿ï¼‰
            # ä¾‹: /run/docs/fit-for-run â†’ /run/docs/
            path = parsed_url.path
            if path.endswith('/'):
                self.base_path = path
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤ã„ã¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿å–å¾—
                self.base_path = '/'.join(path.split('/')[:-1]) + '/'
                if not self.base_path.startswith('/'):
                    self.base_path = '/' + self.base_path
            
            # ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã«ã™ã‚‹
            if self.base_path == '//':
                self.base_path = '/'
            
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (è‡ªå‹•åˆ¤å®š)")
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³: {self.base_domain}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–
        if self.use_javascript:
            self._setup_driver()
    
    def _setup_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            # ChromeDriverã‚’è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†
            service = Service(ChromeDriverManager().install())
            
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸï¼ˆJavaScriptå¯¾å¿œãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
        except Exception as e:
            logger.error(f"Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _close_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
    
    def is_valid_url(self, url: str) -> bool:
        """
        æœ‰åŠ¹ãªURLã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            url: ãƒã‚§ãƒƒã‚¯å¯¾è±¡URL
            
        Returns:
            bool: æœ‰åŠ¹ãªå ´åˆTrue
        """
        parsed = urlparse(url)
        
        # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
        if parsed.netloc != self.base_domain:
            return False
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã‹ãƒã‚§ãƒƒã‚¯
        if not parsed.path.startswith(self.base_path):
            return False
        
        # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        excluded_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.doc', '.docx', '.xls', '.xlsx', '.mp4', '.mp3'}
        path_lower = parsed.path.lower()
        
        for ext in excluded_extensions:
            if path_lower.endswith(ext):
                return False
        
        # é™¤å¤–ãƒ‘ã‚¹
        excluded_paths = {'/admin/', '/api/', '/wp-admin/', '/login/', '/logout/'}
        for excluded_path in excluded_paths:
            if excluded_path in parsed.path:
                return False
        
        return True
    
    def extract_links(self, html_content: str, current_url: str) -> List[str]:
        """
        HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            current_url: ç¾åœ¨ã®ãƒšãƒ¼ã‚¸URL
            
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(current_url, href)
            
            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#sectionï¼‰ã‚’é™¤å»
            parsed = urlparse(absolute_url)
            clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
            
            if self.is_valid_url(clean_url) and clean_url not in self.visited_urls:
                links.append(clean_url)
        
        return links
    
    def extract_text_content(self, html_content: str, url: str) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ã®é™çš„ç‰ˆï¼‰
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        # main, article, div.content ãªã©ã®è¦ç´ ã‚’å„ªå…ˆçš„ã«æ¢ã™
        main_content = None
        for selector in ['main', 'article', '.content', '.main-content', '#content', '#main']:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
        if not main_content:
            main_content = soup.find('body')
        
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
        
        return formatted_content
    
    def extract_text_content_js(self, url: str) -> str:
        """
        Seleniumã§JavaScriptå®Ÿè¡Œå¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        
        Args:
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            logger.info(f"ğŸŒ JavaScriptå®Ÿè¡Œä¸­: {url}")
            self.driver.get(url)
            
            # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†ã®ãŸã‚ï¼‰
            time.sleep(3)
            
            # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            page_source = self.driver.page_source
            
            # BeautifulSoupã§è§£æ
            soup = BeautifulSoup(page_source, 'lxml')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            main_content = None
            for selector in ['main', 'article', '.content', '.main-content', '#content', '#main', '[role="main"]']:
                try:
                    if selector.startswith('.') or selector.startswith('#') or selector.startswith('['):
                        main_content = soup.select_one(selector)
                    else:
                        main_content = soup.find(selector)
                    if main_content:
                        break
                except:
                    continue
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
            
            return formatted_content
            
        except TimeoutException:
            logger.error(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n{'='*50}\n\n"
        except WebDriverException as e:
            logger.error(f"WebDriverã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: {e}\n{'='*50}\n\n"
    
    def scrape_page(self, url: str) -> bool:
        """
        å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
            
        Returns:
            bool: æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            if self.use_javascript:
                # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                content = self.extract_text_content_js(url)
                self.extracted_content.append(content)
                
                # ãƒªãƒ³ã‚¯æŠ½å‡ºã‚‚Seleniumã§å®Ÿè¡Œ
                page_source = self.driver.page_source
                new_links = self.extract_links(page_source, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
                
            else:
                # å¾“æ¥ã®é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                logger.info(f"å–å¾—ä¸­: {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                content = self.extract_text_content(response.text, url)
                self.extracted_content.append(content)
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(response.text, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
            
            return True
            
        except requests.RequestException as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
    
    def discover_all_pages(self) -> List[str]:
        """
        å…¨ãƒšãƒ¼ã‚¸ã®URLã‚’äº‹å‰ã«æ¢ç´¢ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            List[str]: ç™ºè¦‹ã•ã‚ŒãŸå…¨ãƒšãƒ¼ã‚¸URLã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ğŸ“‹ äº‹å‰æ¢ç´¢é–‹å§‹: å…¨ãƒšãƒ¼ã‚¸URLã‚’åé›†ä¸­...")
        
        discovered_urls: Set[str] = set()
        to_explore: List[str] = [self.base_url]
        explored: Set[str] = set()
        
        while to_explore:
            current_url = to_explore.pop(0)
            
            if current_url in explored:
                continue
                
            explored.add(current_url)
            discovered_urls.add(current_url)
            
            try:
                logger.info(f"æ¢ç´¢ä¸­: {current_url}")
                
                if self.use_javascript:
                    # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                    self.driver.get(current_url)
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    page_source = self.driver.page_source
                else:
                    # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    response = self.session.get(current_url, timeout=30)
                    response.raise_for_status()
                    page_source = response.text
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(page_source, current_url)
                for link in new_links:
                    if link not in discovered_urls and link not in to_explore:
                        to_explore.append(link)
                
                # é€²æ—è¡¨ç¤ºï¼ˆ10ã®å€æ•°ã§è¡¨ç¤ºï¼‰
                if len(discovered_urls) % 10 == 0:
                    logger.info(f"ğŸ“Š ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {len(discovered_urls)}ãƒšãƒ¼ã‚¸")
                
            except Exception as e:
                logger.warning(f"æ¢ç´¢ã‚¨ãƒ©ãƒ¼ - {current_url}: {e}")
                continue
            
            # æ¢ç´¢é–“éš”ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚ˆã‚ŠçŸ­ã‚ã«ï¼‰
            time.sleep(self.delay * 0.5)
        
        sorted_urls = sorted(list(discovered_urls))
        logger.info(f"ğŸ“‹ äº‹å‰æ¢ç´¢å®Œäº†: ç·ãƒšãƒ¼ã‚¸æ•° {len(sorted_urls)}ãƒšãƒ¼ã‚¸")
        
        return sorted_urls
    
    def scrape_website(self) -> (int, int):
        """
        Webã‚µã‚¤ãƒˆå…¨ä½“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            (int, int): ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°, å–å¾—ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {self.base_url}")
            logger.info(f"å¯¾è±¡ç¯„å›²: {self.base_domain}{self.base_path}* é…ä¸‹")
            if self.use_javascript:
                logger.info("ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œ")
            
            # äº‹å‰æ¢ç´¢ã§å…¨ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹
            all_pages = self.discover_all_pages()
            total_pages = len(all_pages)
            
            print(f"\nğŸ” äº‹å‰æ¢ç´¢çµæœ:")
            print(f"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸")
            
            if self.max_pages is None:
                pages_to_process = total_pages
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™ãªã—)")
            else:
                pages_to_process = min(total_pages, self.max_pages)
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™: {self.max_pages})")
                if total_pages > self.max_pages:
                    print(f"âš ï¸  æ³¨æ„: {total_pages - self.max_pages}ãƒšãƒ¼ã‚¸ãŒåˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–ã•ã‚Œã¾ã™")
            
            print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: ç´„{pages_to_process * self.delay / 60:.1f}åˆ†")
            print("-" * 60)
            
            # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
            logger.info(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹...")
            
            # å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’è¨­å®š
            self.to_visit_urls = all_pages[:pages_to_process] if self.max_pages else all_pages
            self.visited_urls = set()  # ãƒªã‚»ãƒƒãƒˆ
            
            page_count = 0
            
            while self.to_visit_urls:
                current_url = self.to_visit_urls.pop(0)
                
                if current_url in self.visited_urls:
                    continue
                
                self.visited_urls.add(current_url)
                
                if self.scrape_page(current_url):
                    page_count += 1
                    if self.max_pages is None:
                        logger.info(f"é€²æ—: {page_count}/{total_pages}")
                    else:
                        max_display = min(total_pages, self.max_pages)
                        logger.info(f"é€²æ—: {page_count}/{max_display}")
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
                time.sleep(self.delay)
            
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {page_count}ãƒšãƒ¼ã‚¸å–å¾—")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
            domain_name = self.base_domain.replace('.', '_')
            path_name = self.base_path.replace('/', '_').strip('_')
            if path_name:
                filename_base = f"{domain_name}_{path_name}"
            else:
                filename_base = domain_name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"{filename_base}_all_content_{timestamp}.txt"
            
            # åˆ†å‰²ä¿å­˜ã‚’å®Ÿè¡Œ
            self.save_content_split(base_filename, total_pages, page_count)
            
            return total_pages, page_count
            
        finally:
            # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.use_javascript:
                self._close_driver()
    
    def save_content_split(self, base_filename: str, total_pages: int, pages_processed: int):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜
        
        Args:
            base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
            total_pages: ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°
            pages_processed: å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        if not self.extracted_content:
            logger.warning("ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åˆ†å‰²æ•°ã‚’è¨ˆç®—
        total_files = (len(self.extracted_content) + self.pages_per_file - 1) // self.pages_per_file
        
        logger.info(f"ğŸ“‚ åˆ†å‰²ä¿å­˜é–‹å§‹: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¾ã™")
        print(f"\nğŸ“‚ åˆ†å‰²ä¿å­˜ä¸­...")
        print(f"ğŸ—‚ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²: {self.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ã€è¨ˆ{total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ†é›¢
        base_name = base_filename.replace('.txt', '')
        
        saved_files = []
        
        for file_index in range(total_files):
            start_idx = file_index * self.pages_per_file
            end_idx = min(start_idx + self.pages_per_file, len(self.extracted_content))
            
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            file_content = self.extracted_content[start_idx:end_idx]
            pages_in_file = len(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            filename = f"{base_name}_part{file_index + 1}_of_{total_files}.txt"
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
            domain_name = self.base_domain
            header = f"""NotebookLMç”¨ å…¨ã‚µã‚¤ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ãƒ‘ãƒ¼ãƒˆ {file_index + 1}/{total_files})
ã‚µã‚¤ãƒˆ: {domain_name}
å¯¾è±¡ãƒ‘ã‚¹: {self.base_path}* é…ä¸‹
æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸
å–å¾—ãƒšãƒ¼ã‚¸æ•°: {pages_processed}ãƒšãƒ¼ã‚¸
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: {pages_in_file}ãƒšãƒ¼ã‚¸ (ãƒšãƒ¼ã‚¸{start_idx + 1}ã€œ{end_idx})

{'='*80}

"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµåˆ
            full_content = header + "\n".join(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(full_content)
                
                file_size_kb = len(full_content.encode('utf-8')) / 1024
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({file_size_kb:.1f} KB)")
                saved_files.append(filename)
                
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {filename}: {e}")
        
        # ä¿å­˜çµæœã®è¡¨ç¤º
        print(f"\nâœ… åˆ†å‰²ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}å€‹")
        for i, filename in enumerate(saved_files, 1):
            file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
            print(f"   {i}. {filename} ({file_size_kb:.1f} KB)")
        
        print(f"\nğŸ’¡ NotebookLMã§ã®ä½¿ç”¨æ–¹æ³•:")
        print(f"   å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        print(f"   ãƒ‘ãƒ¼ãƒˆ1ã‹ã‚‰é †ç•ªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™")
    
    def save_content(self, content: str, filename: str):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå¾“æ¥ã®å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼‰
        
        Args:
            content: ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")
            print(f"\nâœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content.encode('utf-8')) / 1024:.1f} KB")
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def process_multiple_urls(url_list: List[str], max_pages: int = None, delay: float = 1.0, 
                         base_path: str = None, pages_per_file: int = 80, use_javascript: bool = False, 
                         exact_urls: bool = False):
    """
    è¤‡æ•°URLã‚’é †æ¬¡å‡¦ç†ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ
    
    Args:
        url_list: å‡¦ç†å¯¾è±¡URLã®ãƒªã‚¹ãƒˆ
        max_pages: 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šã®æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆexact_urlsãŒTrueã®å ´åˆã¯ç„¡è¦–ï¼‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆexact_urlsãŒTrueã®å ´åˆã¯ç„¡è¦–ï¼‰
        pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        exact_urls: Trueã®å ´åˆã€æŒ‡å®šã•ã‚ŒãŸURLã®ã¿ã‚’å‡¦ç†ï¼ˆãƒªãƒ³ã‚¯è¿½è·¡ãªã—ï¼‰
        
    Returns:
        (int, int): ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°ã€ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°
    """
    all_content = []
    total_discovered = 0
    total_processed = 0
    
    if exact_urls:
        print(f"\nğŸ¯ æŒ‡å®šURLé™å®šå‡¦ç†é–‹å§‹: {len(url_list)}ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥å‡¦ç†")
        print("ğŸ“Œ å„URLã®ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—ï¼ˆä¸‹ä½ãƒšãƒ¼ã‚¸ã®è‡ªå‹•åé›†ã¯è¡Œã„ã¾ã›ã‚“ï¼‰")
    else:
        print(f"\nğŸ”— è¤‡æ•°URLå‡¦ç†é–‹å§‹: {len(url_list)}ã‚µã‚¤ãƒˆã‚’é †æ¬¡å‡¦ç†")
        print("ğŸŒ å„URLã‚’èµ·ç‚¹ã¨ã—ã¦ä¸‹ä½ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•åé›†ã—ã¾ã™")
    print("-" * 60)
    
    for i, url in enumerate(url_list, 1):
        print(f"\nğŸ“ [{i}/{len(url_list)}] å‡¦ç†ä¸­: {url}")
        print("-" * 40)
        
        try:
            if exact_urls:
                # æŒ‡å®šURLã®ã¿å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                content = scrape_single_url(url, use_javascript, delay)
                if content:
                    all_content.append(content)
                    total_processed += 1
                    total_discovered += 1
                    print(f"âœ… [{i}/{len(url_list)}] å®Œäº†: 1ãƒšãƒ¼ã‚¸å–å¾—")
                else:
                    print(f"âŒ [{i}/{len(url_list)}] å¤±æ•—: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            else:
                # å¾“æ¥ã®ä¸‹ä½ãƒšãƒ¼ã‚¸è‡ªå‹•åé›†ãƒ¢ãƒ¼ãƒ‰
                scraper = WebsiteScraper(url, max_pages, delay, base_path, pages_per_file, use_javascript)
                discovered, processed = scraper.scrape_website()
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ
                all_content.extend(scraper.extracted_content)
                total_discovered += discovered
                total_processed += processed
                
                print(f"âœ… [{i}/{len(url_list)}] å®Œäº†: {processed}ãƒšãƒ¼ã‚¸å–å¾—")
            
            # ã‚µã‚¤ãƒˆé–“ã®é–“éš”
            if i < len(url_list):
                time.sleep(delay * 2)  # ã‚µã‚¤ãƒˆé–“ã¯é€šå¸¸ã®2å€ã®é–“éš”
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            print(f"âŒ [{i}/{len(url_list)}] ã‚¹ã‚­ãƒƒãƒ—: {url} - {e}")
            continue
    
    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    if all_content:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if exact_urls:
            base_filename = f"exact_urls_content_{timestamp}.txt"
        else:
            base_filename = f"multi_site_content_{timestamp}.txt"
        save_content_split_unified(all_content, base_filename, total_discovered, total_processed, pages_per_file)
    
    return total_discovered, total_processed


def scrape_single_url(url: str, use_javascript: bool = False, delay: float = 1.0) -> str:
    """
    å˜ä¸€URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’å–å¾—ï¼ˆãƒªãƒ³ã‚¯è¿½è·¡ãªã—ï¼‰
    
    Args:
        url: å–å¾—å¯¾è±¡URL
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: é…å»¶æ™‚é–“ï¼ˆJavaScriptå®Ÿè¡Œæ™‚ã®å¾…æ©Ÿã«ä½¿ç”¨ï¼‰
        
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯Noneï¼‰
    """
    try:
        if use_javascript:
            # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            logger.info(f"ğŸŒ JavaScriptå®Ÿè¡Œä¸­: {url}")
            
            # ç°¡æ˜“çš„ãªSeleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.set_page_load_timeout(30)
            
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                
                page_source = driver.page_source
                
                # BeautifulSoupã§è§£æ
                soup = BeautifulSoup(page_source, 'lxml')
                
                # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
                for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    element.decompose()
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title = soup.find('title')
                title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                main_content = None
                for selector in ['main', 'article', '.content', '.main-content', '#content', '#main', '[role="main"]']:
                    try:
                        if selector.startswith('.') or selector.startswith('#') or selector.startswith('['):
                            main_content = soup.select_one(selector)
                        else:
                            main_content = soup.find(selector)
                        if main_content:
                            break
                    except:
                        continue
                
                if not main_content:
                    main_content = soup.find('body')
                
                if main_content:
                    text = main_content.get_text(separator='\n', strip=True)
                else:
                    text = soup.get_text(separator='\n', strip=True)
                
                # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
                text = re.sub(r'\n\s*\n', '\n\n', text)
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
                
                return formatted_content
                
            finally:
                driver.quit()
                
        else:
            # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            logger.info(f"å–å¾—ä¸­: {url}")
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            main_content = None
            for selector in ['main', 'article', '.content', '.main-content', '#content', '#main']:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
            
            return formatted_content
            
    except Exception as e:
        logger.error(f"å˜ä¸€URLå–å¾—ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
        return None


def save_content_split_unified(content_list: List[str], base_filename: str, 
                              total_discovered: int, total_processed: int, pages_per_file: int):
    """
    çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜
    
    Args:
        content_list: å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒªã‚¹ãƒˆ
        base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
        total_discovered: ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°
        total_processed: ç·å‡¦ç†ãƒšãƒ¼ã‚¸æ•°
        pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°
    """
    if not content_list:
        logger.warning("ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # åˆ†å‰²æ•°ã‚’è¨ˆç®—
    total_files = (len(content_list) + pages_per_file - 1) // pages_per_file
    
    logger.info(f"ğŸ“‚ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¾ã™")
    print(f"\nğŸ“‚ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­...")
    print(f"ğŸ—‚ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²: {pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ã€è¨ˆ{total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ†é›¢
    base_name = base_filename.replace('.txt', '')
    
    saved_files = []
    
    for file_index in range(total_files):
        start_idx = file_index * pages_per_file
        end_idx = min(start_idx + pages_per_file, len(content_list))
        
        # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        file_content = content_list[start_idx:end_idx]
        pages_in_file = len(file_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        filename = f"{base_name}_part{file_index + 1}_of_{total_files}.txt"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
        header = f"""NotebookLMç”¨ è¤‡æ•°ã‚µã‚¤ãƒˆçµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ãƒ‘ãƒ¼ãƒˆ {file_index + 1}/{total_files})
æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_discovered}ãƒšãƒ¼ã‚¸
ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: {pages_in_file}ãƒšãƒ¼ã‚¸ (ãƒšãƒ¼ã‚¸{start_idx + 1}ã€œ{end_idx})

{'='*80}

"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµåˆ
        full_content = header + "\n".join(file_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(full_content)
            
            file_size_kb = len(full_content.encode('utf-8')) / 1024
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({file_size_kb:.1f} KB)")
            saved_files.append(filename)
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {filename}: {e}")
    
    # ä¿å­˜çµæœã®è¡¨ç¤º
    print(f"\nâœ… çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†!")
    print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}å€‹")
    for i, filename in enumerate(saved_files, 1):
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        print(f"   {i}. {filename} ({file_size_kb:.1f} KB)")
    
    print(f"\nğŸ’¡ NotebookLMã§ã®ä½¿ç”¨æ–¹æ³•:")
    print(f"   å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    print(f"   ãƒ‘ãƒ¼ãƒˆ1ã‹ã‚‰é †ç•ªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™")


def extract_page_metadata(url: str, use_javascript: bool = False) -> dict:
    """
    å˜ä¸€ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆURLã€titleã€h1ï¼‰ã‚’æŠ½å‡º
    
    Args:
        url: å–å¾—å¯¾è±¡URL
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        
    Returns:
        dict: {'url': str, 'title': str, 'h1': str, 'status': str}
    """
    try:
        if use_javascript:
            # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.set_page_load_timeout(30)
            
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                
                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'lxml')
                
            finally:
                driver.quit()
        else:
            # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
        
        # titleã‚¿ã‚°ã‚’å–å¾—
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else ""
        
        # h1ã‚¿ã‚°ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®ã‚‚ã®ï¼‰
        h1_tag = soup.find('h1')
        h1 = h1_tag.get_text().strip() if h1_tag else ""
        
        return {
            'url': url,
            'title': title,
            'h1': h1,
            'status': 'success'
        }
        
    except Exception as e:
        logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
        return {
            'url': url,
            'title': "",
            'h1': "",
            'status': f'error: {str(e)}'
        }


def process_url_parallel(url: str, base_domain: str, base_path: str, use_javascript: bool, 
                        delay: float, session_data: dict) -> dict:
    """
    ä¸¦åˆ—å‡¦ç†ç”¨ï¼šå˜ä¸€URLã®å‡¦ç†ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º+ãƒªãƒ³ã‚¯åé›†ï¼‰
    
    Args:
        url: å‡¦ç†å¯¾è±¡URL
        base_domain: ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: é…å»¶æ™‚é–“
        session_data: ã‚¹ãƒ¬ãƒƒãƒ‰å…±æœ‰ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        dict: {'metadata': dict, 'new_links': List[str]}
    """
    try:
        if use_javascript:
            # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆå„ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç‹¬ç«‹ã—ãŸãƒ‰ãƒ©ã‚¤ãƒãƒ¼ï¼‰
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.set_page_load_timeout(30)
            
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                page_source = driver.page_source
            finally:
                driver.quit()
        else:
            # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå„ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç‹¬ç«‹ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            page_source = response.text
        
        # BeautifulSoupã§è§£æ
        soup = BeautifulSoup(page_source, 'lxml')
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else ""
        
        h1_tag = soup.find('h1')
        h1 = h1_tag.get_text().strip() if h1_tag else ""
        
        metadata = {
            'url': url,
            'title': title,
            'h1': h1,
            'status': 'success'
        }
        
        # ãƒªãƒ³ã‚¯æŠ½å‡º
        new_links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(url, href)
            
            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#sectionï¼‰ã‚’é™¤å»
            parsed = urlparse(absolute_url)
            clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
            
            # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            if is_valid_url_for_sitemap(clean_url, base_domain, base_path):
                new_links.append(clean_url)
        
        # ä¸¦åˆ—å‡¦ç†ç”¨ã®é…å»¶
        if delay > 0:
            time.sleep(delay)
        
        return {
            'metadata': metadata,
            'new_links': new_links
        }
        
    except Exception as e:
        logger.warning(f"ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
        return {
            'metadata': {
                'url': url,
                'title': "",
                'h1': "",
                'status': f'error: {str(e)}'
            },
            'new_links': []
        }


def save_progress(progress_file: str, discovered_metadata: dict, to_explore: Set[str], 
                 explored: Set[str], base_url: str, base_path: str):
    """
    é€²æ—ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        progress_file: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        discovered_metadata: ç™ºè¦‹æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        to_explore: æ¢ç´¢äºˆå®šURL
        explored: æ¢ç´¢æ¸ˆã¿URL  
        base_url: ãƒ™ãƒ¼ã‚¹URL
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    """
    try:
        progress_data = {
            'timestamp': datetime.now().isoformat(),
            'base_url': base_url,
            'base_path': base_path,
            'discovered_metadata': discovered_metadata,
            'to_explore': list(to_explore),
            'explored': list(explored),
            'total_discovered': len(discovered_metadata),
            'total_explored': len(explored),
            'remaining_to_explore': len(to_explore)
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜: {progress_file} ({len(discovered_metadata)}ãƒšãƒ¼ã‚¸)")
        
    except Exception as e:
        logger.warning(f"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def load_progress(progress_file: str) -> dict:
    """
    é€²æ—ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    
    Args:
        progress_file: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        dict: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ã¾ãŸã¯None
    """
    try:
        if not os.path.exists(progress_file):
            return None
            
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        logger.info(f"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹èª­ã¿è¾¼ã¿: {progress_file}")
        logger.info(f"å‰å›ã®é€²æ—: {progress_data['total_discovered']}ãƒšãƒ¼ã‚¸ç™ºè¦‹æ¸ˆã¿")
        
        return progress_data
        
    except Exception as e:
        logger.warning(f"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def discover_and_extract_sitemap_with_resume(base_url: str, base_path: str = None, use_javascript: bool = False, 
                                           delay: float = 1.0, max_pages: int = None, 
                                           progress_file: str = None, save_interval: int = 50) -> List[dict]:
    """
    ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜ãƒ»å†é–‹å¯¾å¿œç‰ˆï¼š1å›ã®ã‚¢ã‚¯ã‚»ã‚¹ã§URLç™ºè¦‹+ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’åŒæ™‚å®Ÿè¡Œ
    
    Args:
        base_url: åŸºæº–URL
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°åˆ¶é™
        progress_file: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        save_interval: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜é–“éš”ï¼ˆãƒšãƒ¼ã‚¸æ•°ï¼‰
        
    Returns:
        List[dict]: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    logger.info(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å‡¦ç†é–‹å§‹ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜å¯¾å¿œï¼‰: {base_url}")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’è¨­å®š
    parsed_url = urlparse(base_url)
    base_domain = parsed_url.netloc
    
    if base_path is not None:
        if not base_path.startswith('/'):
            base_path = '/' + base_path
        if not base_path.endswith('/'):
            base_path = base_path + '/'
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path} (æ‰‹å‹•æŒ‡å®š)")
    else:
        # è‡ªå‹•åˆ¤å®š
        path = parsed_url.path
        if path.endswith('/'):
            base_path = path
        else:
            base_path = '/'.join(path.split('/')[:-1]) + '/'
            if not base_path.startswith('/'):
                base_path = '/' + base_path
        
        if base_path == '//':
            base_path = '/'
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path} (è‡ªå‹•åˆ¤å®š)")
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹èª­ã¿è¾¼ã¿
    discovered_metadata = {}
    to_explore = set()
    explored = set()
    
    if progress_file:
        progress_data = load_progress(progress_file)
        if progress_data:
            # ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            if (progress_data['base_url'] == base_url and 
                progress_data['base_path'] == base_path):
                
                discovered_metadata = progress_data['discovered_metadata']
                to_explore = set(progress_data['to_explore'])
                explored = set(progress_data['explored'])
                
                print(f"\nğŸ”„ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å†é–‹")
                print(f"ğŸ“Š å‰å›ã®é€²æ—: {len(discovered_metadata)}ãƒšãƒ¼ã‚¸ç™ºè¦‹æ¸ˆã¿")
                print(f"ğŸ“Š æ®‹ã‚Šæ¢ç´¢å¯¾è±¡: {len(to_explore)}URL")
            else:
                print(f"\nâš ï¸  ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®URL/ãƒ‘ã‚¹ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚æ–°è¦é–‹å§‹ã—ã¾ã™ã€‚")
                to_explore.add(base_url)
        else:
            to_explore.add(base_url)
    else:
        to_explore.add(base_url)
    
    print(f"\nğŸš€ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å‡¦ç†é–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡URL: {base_url}")
    print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path}")
    print(f"ğŸŒ ãƒ‰ãƒ¡ã‚¤ãƒ³: {base_domain}")
    if max_pages:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {max_pages}ãƒšãƒ¼ã‚¸")
    else:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™")
    if progress_file:
        print(f"ğŸ’¾ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜: {progress_file} (é–“éš”: {save_interval}ãƒšãƒ¼ã‚¸)")
    if use_javascript:
        print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹")
    else:
        print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
    print(f"â±ï¸  é…å»¶æ™‚é–“: {delay}ç§’")
    print("-" * 50)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    # JavaScriptç”¨ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
    driver = None
    if use_javascript:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(30)
    
    try:
        processed_count = len(discovered_metadata)
        
        while to_explore and (max_pages is None or len(discovered_metadata) < max_pages):
            current_url = list(to_explore)[0]
            to_explore.discard(current_url)
            
            if current_url in explored:
                continue
                
            explored.add(current_url)
            
            print(f"ğŸ” [{len(discovered_metadata) + 1}] å‡¦ç†ä¸­: {current_url}")
            
            try:
                # 1å›ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ãƒšãƒ¼ã‚¸å–å¾—
                if use_javascript:
                    driver.get(current_url)
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                    page_source = driver.page_source
                else:
                    response = session.get(current_url, timeout=30)
                    response.raise_for_status()
                    page_source = response.text
                
                # BeautifulSoupã§è§£æ
                soup = BeautifulSoup(page_source, 'lxml')
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                title_tag = soup.find('title')
                title = title_tag.get_text().strip() if title_tag else ""
                
                h1_tag = soup.find('h1')
                h1 = h1_tag.get_text().strip() if h1_tag else ""
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                discovered_metadata[current_url] = {
                    'url': current_url,
                    'title': title,
                    'h1': h1,
                    'status': 'success'
                }
                
                # ãƒªãƒ³ã‚¯æŠ½å‡º
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    absolute_url = urljoin(current_url, href)
                    
                    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#sectionï¼‰ã‚’é™¤å»
                    parsed = urlparse(absolute_url)
                    clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
                    
                    # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                    if is_valid_url_for_sitemap(clean_url, base_domain, base_path):
                        if clean_url not in discovered_metadata and clean_url not in to_explore and clean_url not in explored:
                            to_explore.add(clean_url)
                
                # é€²æ—è¡¨ç¤º
                title_preview = title[:50] + "..." if len(title) > 50 else title
                print(f"   âœ… Title: {title_preview}")
                print(f"   ğŸ”— æ–°è¦ãƒªãƒ³ã‚¯ç™ºè¦‹: {len(to_explore)}å€‹")
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜ï¼ˆå®šæœŸçš„ï¼‰
                if progress_file and len(discovered_metadata) % save_interval == 0:
                    save_progress(progress_file, discovered_metadata, to_explore, explored, base_url, base_path)
                
                # é€²æ—ã‚µãƒãƒªãƒ¼ï¼ˆ10ã®å€æ•°ã§è¡¨ç¤ºï¼‰
                if len(discovered_metadata) % 10 == 0:
                    print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {len(discovered_metadata)}ãƒšãƒ¼ã‚¸ã€æ®‹ã‚Š: {len(to_explore)}ãƒšãƒ¼ã‚¸")
                
                # ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
                if max_pages and len(discovered_metadata) >= max_pages:
                    print(f"âš ï¸  æœ€å¤§ãƒšãƒ¼ã‚¸æ•°({max_pages})ã«é”ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
            except Exception as e:
                logger.warning(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {current_url}: {e}")
                discovered_metadata[current_url] = {
                    'url': current_url,
                    'title': "",
                    'h1': "",
                    'status': f'error: {str(e)}'
                }
                continue
            
            # é…å»¶
            if to_explore:  # ã¾ã æ¢ç´¢ã™ã‚‹URLãŒã‚ã‚‹å ´åˆã®ã¿
                time.sleep(delay)
    
    finally:
        # æœ€çµ‚ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜
        if progress_file:
            save_progress(progress_file, discovered_metadata, to_explore, explored, base_url, base_path)
        
        # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if driver:
            driver.quit()
    
    # çµæœã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã™
    metadata_list = list(discovered_metadata.values())
    
    print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å‡¦ç†å®Œäº†!")
    print(f"ğŸ“Š ç·å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {len(metadata_list)}")
    print(f"âœ… æˆåŠŸ: {sum(1 for m in metadata_list if m['status'] == 'success')}")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {sum(1 for m in metadata_list if m['status'] != 'success')}")
    
    return metadata_list


def is_valid_url_for_sitemap(url: str, base_domain: str, base_path: str) -> bool:
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆç”¨ã®URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
    
    Args:
        url: ãƒã‚§ãƒƒã‚¯å¯¾è±¡URL
        base_domain: ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        
    Returns:
        bool: æœ‰åŠ¹ãªå ´åˆTrue
    """
    parsed = urlparse(url)
    
    # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
    if parsed.netloc != base_domain:
        return False
    
    # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã‹ãƒã‚§ãƒƒã‚¯
    if not parsed.path.startswith(base_path):
        return False
    
    # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
    excluded_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.doc', '.docx', '.xls', '.xlsx', '.mp4', '.mp3'}
    path_lower = parsed.path.lower()
    
    for ext in excluded_extensions:
        if path_lower.endswith(ext):
            return False
    
    # é™¤å¤–ãƒ‘ã‚¹
    excluded_paths = {'/admin/', '/api/', '/wp-admin/', '/login/', '/logout/'}
    for excluded_path in excluded_paths:
        if excluded_path in parsed.path:
            return False
    
    return True


# generate_sitemapé–¢æ•°ã‚’æ›´æ–°
def generate_sitemap(base_url: str, base_path: str = None, use_javascript: bool = False, 
                    delay: float = 1.0, output_format: str = 'csv', max_workers: int = 1, 
                    max_pages: int = None, progress_file: str = None) -> str:
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼ˆURLã€titleã€h1ã®ãƒªã‚¹ãƒˆï¼‰ã‚’ç”Ÿæˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        base_url: åŸºæº–URL
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        output_format: å‡ºåŠ›å½¢å¼ï¼ˆ'csv' ã¾ãŸã¯ 'txt'ï¼‰
        max_workers: ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆ1ã®å ´åˆã¯é€æ¬¡å‡¦ç†ï¼‰
        max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°åˆ¶é™
        progress_file: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    logger.info(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆé–‹å§‹: {base_url}")
    
    # ğŸš€ æœ€é©åŒ–ï¼šä¸¦åˆ—å‡¦ç† or é€æ¬¡å‡¦ç†ã‚’é¸æŠ
    if max_workers > 1:
        print(f"âš¡ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼")
        metadata_list = discover_and_extract_sitemap_parallel(
            base_url, base_path, use_javascript, delay, max_workers, max_pages
        )
    else:
        print(f"ğŸ”„ é€æ¬¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜å¯¾å¿œ")
        metadata_list = discover_and_extract_sitemap_with_resume(
            base_url, base_path, use_javascript, delay, max_pages, progress_file, 50
        )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    parsed_url = urlparse(base_url)
    domain_name = parsed_url.netloc.replace('.', '_')
    
    # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‹ã‚‰ãƒ‘ã‚¹åã‚’ç”Ÿæˆ
    if base_path and base_path != '/':
        path_name = base_path.replace('/', '_').strip('_')
        filename_base = f"{domain_name}_{path_name}_sitemap_{timestamp}"
    else:
        filename_base = f"{domain_name}_sitemap_{timestamp}"
    
    if output_format.lower() == 'csv':
        filename = f"{filename_base}.csv"
        save_sitemap_csv(metadata_list, filename)
    else:
        filename = f"{filename_base}.txt"
        save_sitemap_txt(metadata_list, filename)
    
    return filename


def save_sitemap_csv(metadata_list: list, filename: str):
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’CSVå½¢å¼ã§ä¿å­˜
    
    Args:
        metadata_list: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    try:
        with open(filename, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            writer.writerow(['URL', 'Title', 'H1', 'Status'])
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for metadata in metadata_list:
                writer.writerow([
                    metadata['url'],
                    metadata['title'],
                    metadata['h1'],
                    metadata['status']
                ])
        
        success_count = sum(1 for m in metadata_list if m['status'] == 'success')
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        
        print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_kb:.1f} KB")
        print(f"ğŸ“Š ç·URLæ•°: {len(metadata_list)}")
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {len(metadata_list) - success_count}")
        
        logger.info(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—CSVä¿å­˜å®Œäº†: {filename}")
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def save_sitemap_txt(metadata_list: list, filename: str):
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’TXTå½¢å¼ã§ä¿å­˜
    
    Args:
        metadata_list: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            f.write("ã‚µã‚¤ãƒˆãƒãƒƒãƒ— - URLãƒ»Titleãƒ»H1 ä¸€è¦§\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç·URLæ•°: {len(metadata_list)}\n")
            f.write("=" * 80 + "\n\n")
            
            # ãƒ‡ãƒ¼ã‚¿
            for i, metadata in enumerate(metadata_list, 1):
                f.write(f"{i}. URL: {metadata['url']}\n")
                f.write(f"   Title: {metadata['title']}\n")
                f.write(f"   H1: {metadata['h1']}\n")
                f.write(f"   Status: {metadata['status']}\n")
                f.write("-" * 40 + "\n")
        
        success_count = sum(1 for m in metadata_list if m['status'] == 'success')
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        
        print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_kb:.1f} KB")
        print(f"ğŸ“Š ç·URLæ•°: {len(metadata_list)}")
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {len(metadata_list) - success_count}")
        
        logger.info(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—TXTä¿å­˜å®Œäº†: {filename}")
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—TXTä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def discover_and_extract_sitemap_parallel(base_url: str, base_path: str = None, use_javascript: bool = False, 
                                         delay: float = 1.0, max_workers: int = 5, max_pages: int = None) -> List[dict]:
    """
    ä¸¦åˆ—å‡¦ç†ç‰ˆï¼š1å›ã®ã‚¢ã‚¯ã‚»ã‚¹ã§URLç™ºè¦‹+ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’åŒæ™‚å®Ÿè¡Œ
    
    Args:
        base_url: åŸºæº–URL
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        max_workers: ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°åˆ¶é™
        
    Returns:
        List[dict]: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    logger.info(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¸¦åˆ—å‡¦ç†é–‹å§‹: {base_url} (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers})")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’è¨­å®š
    parsed_url = urlparse(base_url)
    base_domain = parsed_url.netloc
    
    if base_path is not None:
        if not base_path.startswith('/'):
            base_path = '/' + base_path
        if not base_path.endswith('/'):
            base_path = base_path + '/'
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path} (æ‰‹å‹•æŒ‡å®š)")
    else:
        # è‡ªå‹•åˆ¤å®š
        path = parsed_url.path
        if path.endswith('/'):
            base_path = path
        else:
            base_path = '/'.join(path.split('/')[:-1]) + '/'
            if not base_path.startswith('/'):
                base_path = '/' + base_path
        
        if base_path == '//':
            base_path = '/'
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path} (è‡ªå‹•åˆ¤å®š)")
    
    print(f"\nğŸš€ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¸¦åˆ—å‡¦ç†é–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡URL: {base_url}")
    print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {base_path}")
    print(f"ğŸŒ ãƒ‰ãƒ¡ã‚¤ãƒ³: {base_domain}")
    print(f"âš¡ ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
    if max_pages:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {max_pages}ãƒšãƒ¼ã‚¸")
    else:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™")
    if use_javascript:
        print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹")
    else:
        print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
    print(f"â±ï¸  é…å»¶æ™‚é–“: {delay}ç§’")
    print("-" * 50)
    
    # æ¢ç´¢ç”¨å¤‰æ•°
    discovered_metadata: dict = {}  # URL -> metadata
    to_explore: Set[str] = {base_url}
    explored: Set[str] = set()
    
    while to_explore and (max_pages is None or len(discovered_metadata) < max_pages):
        # ç¾åœ¨ã®ãƒãƒƒãƒã‚’æº–å‚™ï¼ˆãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’è€ƒæ…®ï¼‰
        remaining_pages = max_pages - len(discovered_metadata) if max_pages else None
        batch_size = max_workers * 2
        
        if remaining_pages and remaining_pages < batch_size:
            batch_size = remaining_pages
        
        current_batch = list(to_explore)[:batch_size]
        batch_urls = []
        
        for url in current_batch:
            if url not in explored:
                batch_urls.append(url)
                explored.add(url)
                to_explore.discard(url)
        
        if not batch_urls:
            break
        
        print(f"\nğŸ“¦ ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†: {len(batch_urls)}URLï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}ã€æ®‹ã‚Š: {len(to_explore)}URLï¼‰")
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
            future_to_url = {
                executor.submit(process_url_parallel, url, base_domain, base_path, 
                              use_javascript, delay, {}): url 
                for url in batch_urls
            }
            
            # çµæœã‚’åé›†
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                
                try:
                    result = future.result()
                    metadata = result['metadata']
                    new_links = result['new_links']
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    discovered_metadata[url] = metadata
                    
                    # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢å¯¾è±¡ã«è¿½åŠ ï¼ˆãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’è€ƒæ…®ï¼‰
                    if max_pages is None or len(discovered_metadata) < max_pages:
                        for link in new_links:
                            if link not in discovered_metadata and link not in explored:
                                to_explore.add(link)
                    
                    # é€²æ—è¡¨ç¤º
                    if metadata['status'] == 'success':
                        title_preview = metadata['title'][:40] + "..." if len(metadata['title']) > 40 else metadata['title']
                        print(f"   âš¡ {url} â†’ {title_preview}")
                    else:
                        print(f"   âŒ {url} â†’ {metadata['status']}")
                
                except Exception as e:
                    logger.error(f"ä¸¦åˆ—å‡¦ç†çµæœå–å¾—ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
                    discovered_metadata[url] = {
                        'url': url,
                        'title': "",
                        'h1': "",
                        'status': f'processing_error: {str(e)}'
                    }
        
        # ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if max_pages and len(discovered_metadata) >= max_pages:
            print(f"âš ï¸  æœ€å¤§ãƒšãƒ¼ã‚¸æ•°({max_pages})ã«é”ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        
        # ãƒãƒƒãƒé–“ã®é…å»¶
        if to_explore:
            time.sleep(delay * 0.5)  # ä¸¦åˆ—å‡¦ç†ã§ã¯çŸ­ç¸®
            
        # é€²æ—ã‚µãƒãƒªãƒ¼
        print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {len(discovered_metadata)}ãƒšãƒ¼ã‚¸ã€ç™ºè¦‹æ¸ˆã¿: {len(to_explore)}ãƒšãƒ¼ã‚¸")
    
    # çµæœã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã™
    metadata_list = list(discovered_metadata.values())
    
    print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¸¦åˆ—å‡¦ç†å®Œäº†!")
    print(f"ğŸ“Š ç·å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {len(metadata_list)}")
    print(f"âœ… æˆåŠŸ: {sum(1 for m in metadata_list if m['status'] == 'success')}")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {sum(1 for m in metadata_list if m['status'] != 'success')}")
    print(f"âš¡ ä¸¦åˆ—åŒ–åŠ¹æœ: ç´„{max_workers}å€ã®é«˜é€ŸåŒ–")
    
    return metadata_list


def main():
    parser = argparse.ArgumentParser(description='NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«')
    parser.add_argument('url', nargs='?', help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆURLï¼ˆ--url-listã¨æ’ä»–çš„ï¼‰')
    parser.add_argument('--url-list', type=str, help='URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰')
    parser.add_argument('--max-pages', type=int, default=1000, help='æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰')
    parser.add_argument('--delay', type=float, default=1.0, help='ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰')
    parser.add_argument('--base-path', type=str, default=None, help='ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆä¾‹: /run/docs/ï¼‰æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®š')
    parser.add_argument('--no-limit', action='store_true', help='ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆæ³¨æ„: å¤§é‡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰')
    parser.add_argument('--pages-per-file', type=int, default=80, help='1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 80ï¼‰NotebookLMã®åˆ¶é™ã«å¿œã˜ã¦èª¿æ•´')
    parser.add_argument('--javascript', action='store_true', help='JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œã€å‡¦ç†æ™‚é–“ãŒé•·ããªã‚Šã¾ã™ï¼‰')
    parser.add_argument('--exact-urls', action='store_true', help='æŒ‡å®šã•ã‚ŒãŸURLãƒªã‚¹ãƒˆã®URLã®ã¿ã‚’å‡¦ç†ã—ã€ãƒªãƒ³ã‚¯è¿½è·¡ã‚’è¡Œã‚ãªã„')
    parser.add_argument('--generate-sitemap', action='store_true', help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼ˆURLãƒ»titleãƒ»h1ã®ãƒªã‚¹ãƒˆï¼‰ã‚’ç”Ÿæˆã™ã‚‹')
    parser.add_argument('--sitemap-format', type=str, choices=['csv', 'txt'], default='csv', help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å‡ºåŠ›å½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: csvï¼‰')
    parser.add_argument('--parallel-workers', type=int, default=1, help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆæ™‚ã®ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1=é€æ¬¡å‡¦ç†ï¼‰')
    parser.add_argument('--max-sitemap-pages', type=int, default=None, help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆæ™‚ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åˆ¶é™ï¼‰')
    parser.add_argument('--resume-from', type=str, help='ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†é–‹ï¼ˆä¾‹: progress.jsonï¼‰')
    parser.add_argument('--save-progress', type=str, help='ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: progress.jsonï¼‰')
    
    args = parser.parse_args()
    
    # URL ã¾ãŸã¯ URL-list ã®æ’ä»–çš„ãƒã‚§ãƒƒã‚¯
    if not args.url and not args.url_list:
        parser.error("URLã¾ãŸã¯--url-listã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    if args.url and args.url_list:
        parser.error("URLã¨--url-listã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    
    # --exact-urls ã¯ --url-list ã¨çµ„ã¿åˆã‚ã›ã¦ã®ã¿ä½¿ç”¨å¯èƒ½
    if args.exact_urls and not args.url_list:
        parser.error("--exact-urlsã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯--url-listã¨çµ„ã¿åˆã‚ã›ã¦ã®ã¿ä½¿ç”¨ã§ãã¾ã™")
    
    # --generate-sitemap ã¯å˜ä¸€URLã¨ã®ã¿çµ„ã¿åˆã‚ã›å¯èƒ½
    if args.generate_sitemap and args.url_list:
        parser.error("--generate-sitemapã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å˜ä¸€URLã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™")
    
    if args.generate_sitemap and args.exact_urls:
        parser.error("--generate-sitemapã¨--exact-urlsã¯åŒæ™‚ã«ä½¿ç”¨ã§ãã¾ã›ã‚“")
    
    # --parallel-workersã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    if args.parallel_workers < 1 or args.parallel_workers > 20:
        parser.error("--parallel-workersã¯1-20ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # --max-sitemap-pagesã®ç¯„å›²ãƒã‚§ãƒƒã‚¯  
    if args.max_sitemap_pages is not None and args.max_sitemap_pages < 1:
        parser.error("--max-sitemap-pagesã¯1ä»¥ä¸Šã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # --no-limitãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ç„¡åˆ¶é™ã«
    max_pages = None if args.no_limit else args.max_pages
    
    if args.generate_sitemap:
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
        print(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ å¯¾è±¡URL: {args.url}")
        if args.base_path:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: è‡ªå‹•åˆ¤å®š")
        print(f"ğŸ“Š å‡ºåŠ›å½¢å¼: {args.sitemap_format.upper()}")
        if args.max_sitemap_pages:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_sitemap_pages}ãƒšãƒ¼ã‚¸")
        else:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™")
        if args.save_progress or args.resume_from:
            progress_file = args.save_progress or args.resume_from
            print(f"ğŸ’¾ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä¿å­˜: {progress_file}")
        if args.resume_from:
            print(f"ğŸ”„ å†é–‹ãƒ¢ãƒ¼ãƒ‰: {args.resume_from}ã‹ã‚‰å†é–‹")
        print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
        if args.javascript:
            print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
        else:
            print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
        print("-" * 50)
        
        try:
            progress_file = args.save_progress or args.resume_from
            filename = generate_sitemap(args.url, args.base_path, args.javascript, 
                                       args.delay, args.sitemap_format, args.parallel_workers, 
                                       args.max_sitemap_pages, progress_file)
            print(f"\nğŸ‰ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆå®Œäº†ï¼")
            print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"ğŸ’¡ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚µã‚¤ãƒˆå…¨ä½“ã®æ§‹é€ ã‚’ç¢ºèªã§ãã¾ã™")
            
        except Exception as e:
            logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return
    
    elif args.url_list:
        # URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å‡¦ç†
        try:
            url_list = load_urls_from_file(args.url_list)
            
            print(f"ğŸš€ è¤‡æ•°Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            print(f"ğŸ“‹ URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {args.url_list}")
            print(f"ğŸ“Š å¯¾è±¡ã‚µã‚¤ãƒˆæ•°: {len(url_list)}ã‚µã‚¤ãƒˆ")
            if args.exact_urls:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: ç„¡è¦–ï¼ˆå„URLã‚’ç›´æ¥å‡¦ç†ï¼‰")
            elif args.base_path:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
            else:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: å„ã‚µã‚¤ãƒˆã§è‡ªå‹•åˆ¤å®š")
            if args.exact_urls:
                print(f"ğŸ“Š ãƒšãƒ¼ã‚¸æ•°åˆ¶é™: å„URL1ãƒšãƒ¼ã‚¸ã®ã¿ï¼ˆ--max-pagesè¨­å®šã¯ç„¡è¦–ï¼‰")
            elif args.no_limit:
                print(f"ğŸ“Š 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™ âš ï¸")
            else:
                print(f"ğŸ“Š 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
            print(f"ğŸ—‚ï¸  åˆ†å‰²è¨­å®š: {args.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²")
            print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
            if args.javascript:
                print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
            else:
                print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
            if args.exact_urls:
                print(f"ğŸ¯ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: æŒ‡å®šURLé™å®š (å„URLã®ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†)")
            else:
                print(f"ğŸŒ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ã‚µã‚¤ãƒˆå…¨ä½“åé›† (å„URLã‹ã‚‰ä¸‹ä½ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•åé›†)")
            print("-" * 50)
            
            total_discovered, total_processed = process_multiple_urls(
                url_list, max_pages, args.delay, args.base_path, args.pages_per_file, args.javascript, args.exact_urls
            )
            
            if args.exact_urls:
                print("\nğŸ‰ æŒ‡å®šURLé™å®šå‡¦ç†å®Œäº†ï¼")
                print(f"ğŸ“¥ å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸")
                print(f"NotebookLMã«æŒ‡å®šã•ã‚ŒãŸ{total_processed}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")
            else:
                print("\nğŸ‰ è¤‡æ•°ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
                print(f"ğŸ“Š ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_discovered}ãƒšãƒ¼ã‚¸")
                print(f"ğŸ“¥ ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸")
                print(f"NotebookLMã«çµ±åˆã•ã‚ŒãŸ{total_processed}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")
            
        except Exception as e:
            logger.error(f"URLãƒªã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return
    
    else:
        # å˜ä¸€URLã‹ã‚‰ã®å‡¦ç†ï¼ˆæ—¢å­˜ã®å‡¦ç†ï¼‰
        print(f"ğŸš€ Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        print(f"ğŸ“ å¯¾è±¡URL: {args.url}")
        if args.base_path:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: è‡ªå‹•åˆ¤å®š")
        if args.no_limit:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™ âš ï¸")
        else:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
        print(f"ğŸ—‚ï¸  åˆ†å‰²è¨­å®š: {args.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²")
        print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
        if args.javascript:
            print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
        else:
            print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
        print("-" * 50)
        
        scraper = WebsiteScraper(args.url, max_pages, args.delay, args.base_path, args.pages_per_file, args.javascript)
        total_pages, page_count = scraper.scrape_website()
        
        print("\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
        print(f"NotebookLMã« {page_count}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
