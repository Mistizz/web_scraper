#!/usr/bin/env python3
"""
NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«
æŒ‡å®šã•ã‚ŒãŸWebã‚µã‚¤ãƒˆã®å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã¾ã™ã€‚
JavaScriptå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œã€‚
"""

import argparse
import requests
import time
import re
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import BeautifulSoup
from typing import Set, List
from datetime import datetime
import logging

# Seleniumé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebsiteScraper:
    def __init__(self, base_url: str, max_pages: int = 1000, delay: float = 1.0, 
                 base_path: str = None, pages_per_file: int = 80, use_javascript: bool = False):
        """
        Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆNoneã®å ´åˆã¯ç„¡åˆ¶é™ï¼‰
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
            base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆåˆ†å‰²å‡ºåŠ›ç”¨ï¼‰
            use_javascript: JavaScriptã‚’å®Ÿè¡Œã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.pages_per_file = pages_per_file
        self.use_javascript = use_javascript
        self.driver = None
        self.visited_urls: Set[str] = set()
        self.to_visit_urls: List[str] = [base_url]
        self.extracted_content: List[str] = []
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’å–å¾—
        parsed_url = urlparse(base_url)
        self.base_domain = parsed_url.netloc
        
        if base_path is not None:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå ´åˆ
            self.base_path = base_path
            if not self.base_path.startswith('/'):
                self.base_path = '/' + self.base_path
            if not self.base_path.endswith('/'):
                self.base_path = self.base_path + '/'
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿ï¼‰
            # ä¾‹: /run/docs/fit-for-run â†’ /run/docs/
            path = parsed_url.path
            if path.endswith('/'):
                self.base_path = path
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤ã„ã¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿å–å¾—
                self.base_path = '/'.join(path.split('/')[:-1]) + '/'
                if not self.base_path.startswith('/'):
                    self.base_path = '/' + self.base_path
            
            # ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã«ã™ã‚‹
            if self.base_path == '//':
                self.base_path = '/'
            
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (è‡ªå‹•åˆ¤å®š)")
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³: {self.base_domain}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–
        if self.use_javascript:
            self._setup_driver()
    
    def _setup_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            # ChromeDriverã‚’è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†
            service = Service(ChromeDriverManager().install())
            
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸï¼ˆJavaScriptå¯¾å¿œãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
        except Exception as e:
            logger.error(f"Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _close_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
    
    def is_valid_url(self, url: str) -> bool:
        """
        æœ‰åŠ¹ãªURLã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            url: ãƒã‚§ãƒƒã‚¯å¯¾è±¡URL
            
        Returns:
            bool: æœ‰åŠ¹ãªå ´åˆTrue
        """
        parsed = urlparse(url)
        
        # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
        if parsed.netloc != self.base_domain:
            return False
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã‹ãƒã‚§ãƒƒã‚¯
        if not parsed.path.startswith(self.base_path):
            return False
        
        # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        excluded_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.doc', '.docx', '.xls', '.xlsx', '.mp4', '.mp3'}
        path_lower = parsed.path.lower()
        
        for ext in excluded_extensions:
            if path_lower.endswith(ext):
                return False
        
        # é™¤å¤–ãƒ‘ã‚¹
        excluded_paths = {'/admin/', '/api/', '/wp-admin/', '/login/', '/logout/'}
        for excluded_path in excluded_paths:
            if excluded_path in parsed.path:
                return False
        
        return True
    
    def extract_links(self, html_content: str, current_url: str) -> List[str]:
        """
        HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            current_url: ç¾åœ¨ã®ãƒšãƒ¼ã‚¸URL
            
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(current_url, href)
            
            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#sectionï¼‰ã‚’é™¤å»
            parsed = urlparse(absolute_url)
            clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
            
            if self.is_valid_url(clean_url) and clean_url not in self.visited_urls:
                links.append(clean_url)
        
        return links
    
    def extract_text_content(self, html_content: str, url: str) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ã®é™çš„ç‰ˆï¼‰
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        # main, article, div.content ãªã©ã®è¦ç´ ã‚’å„ªå…ˆçš„ã«æ¢ã™
        main_content = None
        for selector in ['main', 'article', '.content', '.main-content', '#content', '#main']:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
        if not main_content:
            main_content = soup.find('body')
        
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
        
        return formatted_content
    
    def extract_text_content_js(self, url: str) -> str:
        """
        Seleniumã§JavaScriptå®Ÿè¡Œå¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        
        Args:
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            logger.info(f"ğŸŒ JavaScriptå®Ÿè¡Œä¸­: {url}")
            self.driver.get(url)
            
            # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†ã®ãŸã‚ï¼‰
            time.sleep(3)
            
            # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            page_source = self.driver.page_source
            
            # BeautifulSoupã§è§£æ
            soup = BeautifulSoup(page_source, 'lxml')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            main_content = None
            for selector in ['main', 'article', '.content', '.main-content', '#content', '#main', '[role="main"]']:
                try:
                    if selector.startswith('.') or selector.startswith('#') or selector.startswith('['):
                        main_content = soup.select_one(selector)
                    else:
                        main_content = soup.find(selector)
                    if main_content:
                        break
                except:
                    continue
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
            
            return formatted_content
            
        except TimeoutException:
            logger.error(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n{'='*50}\n\n"
        except WebDriverException as e:
            logger.error(f"WebDriverã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: {e}\n{'='*50}\n\n"
    
    def scrape_page(self, url: str) -> bool:
        """
        å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
            
        Returns:
            bool: æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            if self.use_javascript:
                # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                content = self.extract_text_content_js(url)
                self.extracted_content.append(content)
                
                # ãƒªãƒ³ã‚¯æŠ½å‡ºã‚‚Seleniumã§å®Ÿè¡Œ
                page_source = self.driver.page_source
                new_links = self.extract_links(page_source, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
                
            else:
                # å¾“æ¥ã®é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                logger.info(f"å–å¾—ä¸­: {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                content = self.extract_text_content(response.text, url)
                self.extracted_content.append(content)
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(response.text, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
            
            return True
            
        except requests.RequestException as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
    
    def discover_all_pages(self) -> List[str]:
        """
        å…¨ãƒšãƒ¼ã‚¸ã®URLã‚’äº‹å‰ã«æ¢ç´¢ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            List[str]: ç™ºè¦‹ã•ã‚ŒãŸå…¨ãƒšãƒ¼ã‚¸URLã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ğŸ“‹ äº‹å‰æ¢ç´¢é–‹å§‹: å…¨ãƒšãƒ¼ã‚¸URLã‚’åé›†ä¸­...")
        
        discovered_urls: Set[str] = set()
        to_explore: List[str] = [self.base_url]
        explored: Set[str] = set()
        
        while to_explore:
            current_url = to_explore.pop(0)
            
            if current_url in explored:
                continue
                
            explored.add(current_url)
            discovered_urls.add(current_url)
            
            try:
                logger.info(f"æ¢ç´¢ä¸­: {current_url}")
                
                if self.use_javascript:
                    # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                    self.driver.get(current_url)
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    page_source = self.driver.page_source
                else:
                    # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    response = self.session.get(current_url, timeout=30)
                    response.raise_for_status()
                    page_source = response.text
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(page_source, current_url)
                for link in new_links:
                    if link not in discovered_urls and link not in to_explore:
                        to_explore.append(link)
                
                # é€²æ—è¡¨ç¤ºï¼ˆ10ã®å€æ•°ã§è¡¨ç¤ºï¼‰
                if len(discovered_urls) % 10 == 0:
                    logger.info(f"ğŸ“Š ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {len(discovered_urls)}ãƒšãƒ¼ã‚¸")
                
            except Exception as e:
                logger.warning(f"æ¢ç´¢ã‚¨ãƒ©ãƒ¼ - {current_url}: {e}")
                continue
            
            # æ¢ç´¢é–“éš”ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚ˆã‚ŠçŸ­ã‚ã«ï¼‰
            time.sleep(self.delay * 0.5)
        
        sorted_urls = sorted(list(discovered_urls))
        logger.info(f"ğŸ“‹ äº‹å‰æ¢ç´¢å®Œäº†: ç·ãƒšãƒ¼ã‚¸æ•° {len(sorted_urls)}ãƒšãƒ¼ã‚¸")
        
        return sorted_urls
    
    def scrape_website(self) -> (int, int):
        """
        Webã‚µã‚¤ãƒˆå…¨ä½“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            (int, int): ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°, å–å¾—ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {self.base_url}")
            logger.info(f"å¯¾è±¡ç¯„å›²: {self.base_domain}{self.base_path}* é…ä¸‹")
            if self.use_javascript:
                logger.info("ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œ")
            
            # äº‹å‰æ¢ç´¢ã§å…¨ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹
            all_pages = self.discover_all_pages()
            total_pages = len(all_pages)
            
            print(f"\nğŸ” äº‹å‰æ¢ç´¢çµæœ:")
            print(f"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸")
            
            if self.max_pages is None:
                pages_to_process = total_pages
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™ãªã—)")
            else:
                pages_to_process = min(total_pages, self.max_pages)
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™: {self.max_pages})")
                if total_pages > self.max_pages:
                    print(f"âš ï¸  æ³¨æ„: {total_pages - self.max_pages}ãƒšãƒ¼ã‚¸ãŒåˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–ã•ã‚Œã¾ã™")
            
            print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: ç´„{pages_to_process * self.delay / 60:.1f}åˆ†")
            print("-" * 60)
            
            # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
            logger.info(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹...")
            
            # å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’è¨­å®š
            self.to_visit_urls = all_pages[:pages_to_process] if self.max_pages else all_pages
            self.visited_urls = set()  # ãƒªã‚»ãƒƒãƒˆ
            
            page_count = 0
            
            while self.to_visit_urls:
                current_url = self.to_visit_urls.pop(0)
                
                if current_url in self.visited_urls:
                    continue
                
                self.visited_urls.add(current_url)
                
                if self.scrape_page(current_url):
                    page_count += 1
                    if self.max_pages is None:
                        logger.info(f"é€²æ—: {page_count}/{total_pages}")
                    else:
                        max_display = min(total_pages, self.max_pages)
                        logger.info(f"é€²æ—: {page_count}/{max_display}")
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
                time.sleep(self.delay)
            
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {page_count}ãƒšãƒ¼ã‚¸å–å¾—")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
            domain_name = self.base_domain.replace('.', '_')
            path_name = self.base_path.replace('/', '_').strip('_')
            if path_name:
                filename_base = f"{domain_name}_{path_name}"
            else:
                filename_base = domain_name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"{filename_base}_all_content_{timestamp}.txt"
            
            # åˆ†å‰²ä¿å­˜ã‚’å®Ÿè¡Œ
            self.save_content_split(base_filename, total_pages, page_count)
            
            return total_pages, page_count
            
        finally:
            # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.use_javascript:
                self._close_driver()
    
    def save_content_split(self, base_filename: str, total_pages: int, pages_processed: int):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜
        
        Args:
            base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
            total_pages: ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°
            pages_processed: å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        if not self.extracted_content:
            logger.warning("ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åˆ†å‰²æ•°ã‚’è¨ˆç®—
        total_files = (len(self.extracted_content) + self.pages_per_file - 1) // self.pages_per_file
        
        logger.info(f"ğŸ“‚ åˆ†å‰²ä¿å­˜é–‹å§‹: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¾ã™")
        print(f"\nğŸ“‚ åˆ†å‰²ä¿å­˜ä¸­...")
        print(f"ğŸ—‚ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²: {self.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ã€è¨ˆ{total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ†é›¢
        base_name = base_filename.replace('.txt', '')
        
        saved_files = []
        
        for file_index in range(total_files):
            start_idx = file_index * self.pages_per_file
            end_idx = min(start_idx + self.pages_per_file, len(self.extracted_content))
            
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            file_content = self.extracted_content[start_idx:end_idx]
            pages_in_file = len(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            filename = f"{base_name}_part{file_index + 1}_of_{total_files}.txt"
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
            domain_name = self.base_domain
            header = f"""NotebookLMç”¨ å…¨ã‚µã‚¤ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ãƒ‘ãƒ¼ãƒˆ {file_index + 1}/{total_files})
ã‚µã‚¤ãƒˆ: {domain_name}
å¯¾è±¡ãƒ‘ã‚¹: {self.base_path}* é…ä¸‹
æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸
å–å¾—ãƒšãƒ¼ã‚¸æ•°: {pages_processed}ãƒšãƒ¼ã‚¸
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: {pages_in_file}ãƒšãƒ¼ã‚¸ (ãƒšãƒ¼ã‚¸{start_idx + 1}ã€œ{end_idx})

{'='*80}

"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµåˆ
            full_content = header + "\n".join(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(full_content)
                
                file_size_kb = len(full_content.encode('utf-8')) / 1024
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({file_size_kb:.1f} KB)")
                saved_files.append(filename)
                
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {filename}: {e}")
        
        # ä¿å­˜çµæœã®è¡¨ç¤º
        print(f"\nâœ… åˆ†å‰²ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}å€‹")
        for i, filename in enumerate(saved_files, 1):
            file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
            print(f"   {i}. {filename} ({file_size_kb:.1f} KB)")
        
        print(f"\nğŸ’¡ NotebookLMã§ã®ä½¿ç”¨æ–¹æ³•:")
        print(f"   å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        print(f"   ãƒ‘ãƒ¼ãƒˆ1ã‹ã‚‰é †ç•ªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™")
    
    def save_content(self, content: str, filename: str):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå¾“æ¥ã®å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼‰
        
        Args:
            content: ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")
            print(f"\nâœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content.encode('utf-8')) / 1024:.1f} KB")
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def main():
    parser = argparse.ArgumentParser(description='NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«')
    parser.add_argument('url', help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆURL')
    parser.add_argument('--max-pages', type=int, default=1000, help='æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰')
    parser.add_argument('--delay', type=float, default=1.0, help='ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰')
    parser.add_argument('--base-path', type=str, default=None, help='ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆä¾‹: /run/docs/ï¼‰æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®š')
    parser.add_argument('--no-limit', action='store_true', help='ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆæ³¨æ„: å¤§é‡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰')
    parser.add_argument('--pages-per-file', type=int, default=80, help='1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 80ï¼‰NotebookLMã®åˆ¶é™ã«å¿œã˜ã¦èª¿æ•´')
    parser.add_argument('--javascript', action='store_true', help='JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œã€å‡¦ç†æ™‚é–“ãŒé•·ããªã‚Šã¾ã™ï¼‰')
    
    args = parser.parse_args()
    
    # --no-limitãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ç„¡åˆ¶é™ã«
    max_pages = None if args.no_limit else args.max_pages
    
    print(f"ğŸš€ Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡URL: {args.url}")
    if args.base_path:
        print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
    else:
        print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: è‡ªå‹•åˆ¤å®š")
    if args.no_limit:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™ âš ï¸")
    else:
        print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
    print(f"ğŸ—‚ï¸  åˆ†å‰²è¨­å®š: {args.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²")
    print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
    if args.javascript:
        print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
    else:
        print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
    print("-" * 50)
    
    scraper = WebsiteScraper(args.url, max_pages, args.delay, args.base_path, args.pages_per_file, args.javascript)
    total_pages, page_count = scraper.scrape_website()
    
    print("\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
    print(f"NotebookLMã« {page_count}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
