#!/usr/bin/env python3
"""
NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«
æŒ‡å®šã•ã‚ŒãŸWebã‚µã‚¤ãƒˆã®å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã¾ã™ã€‚
JavaScriptå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œã€‚
"""

import argparse
import requests
import time
import re
import csv
import os
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import BeautifulSoup
from typing import Set, List
from datetime import datetime
import logging

# Seleniumé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_urls_from_file(file_path: str) -> List[str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰
        
    Returns:
        List[str]: URLã®ãƒªã‚¹ãƒˆ
    """
    urls = []
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    
    file_extension = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_extension == '.txt':
            # txtãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼š1è¡Œ1URL
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):  # ç©ºè¡Œã¨ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤å¤–
                        if line.startswith('http://') or line.startswith('https://'):
                            urls.append(line)
                        else:
                            logger.warning(f"ç„¡åŠ¹ãªURLï¼ˆè¡Œ{line_num}ï¼‰: {line}")
        
        elif file_extension == '.csv':
            # csvãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼š1åˆ—ç›®ãŒURLã€ã¾ãŸã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã§'url'åˆ—ã‚’æŒ‡å®š
            with open(file_path, 'r', encoding='utf-8') as f:
                # æœ€åˆã®è¡Œã‚’ç¢ºèªã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã©ã†ã‹åˆ¤å®š
                first_line = f.readline().strip()
                f.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«æˆ»ã‚‹
                
                reader = csv.reader(f)
                headers = next(reader)  # æœ€åˆã®è¡Œã‚’èª­ã‚€
                
                # 'url'åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                url_column_index = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1åˆ—ç›®
                if 'url' in [h.lower() for h in headers]:
                    url_column_index = [h.lower() for h in headers].index('url')
                elif not (first_line.startswith('http://') or first_line.startswith('https://')):
                    # æœ€åˆã®è¡ŒãŒURLã§ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ã—ã¦æ‰±ã†
                    pass  # url_column_indexã¯0ã®ã¾ã¾
                else:
                    # æœ€åˆã®è¡ŒãŒURLã®å ´åˆã¯ã€ãã‚Œã‚‚å‡¦ç†å¯¾è±¡ã«å«ã‚ã‚‹
                    f.seek(0)
                    reader = csv.reader(f)
                
                for row_num, row in enumerate(reader, 1):
                    if row and len(row) > url_column_index:
                        url = row[url_column_index].strip()
                        if url and (url.startswith('http://') or url.startswith('https://')):
                            urls.append(url)
                        elif url:
                            logger.warning(f"ç„¡åŠ¹ãªURLï¼ˆè¡Œ{row_num}ï¼‰: {url}")
        
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")
    
    except Exception as e:
        logger.error(f"URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    if not urls:
        raise ValueError("æœ‰åŠ¹ãªURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    logger.info(f"URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{len(urls)}å€‹ã®URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    return urls


class WebsiteScraper:
    def __init__(self, base_url: str, max_pages: int = 1000, delay: float = 1.0, 
                 base_path: str = None, pages_per_file: int = 80, use_javascript: bool = False):
        """
        Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆNoneã®å ´åˆã¯ç„¡åˆ¶é™ï¼‰
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
            base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆåˆ†å‰²å‡ºåŠ›ç”¨ï¼‰
            use_javascript: JavaScriptã‚’å®Ÿè¡Œã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.pages_per_file = pages_per_file
        self.use_javascript = use_javascript
        self.driver = None
        self.visited_urls: Set[str] = set()
        self.to_visit_urls: List[str] = [base_url]
        self.extracted_content: List[str] = []
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’å–å¾—
        parsed_url = urlparse(base_url)
        self.base_domain = parsed_url.netloc
        
        if base_path is not None:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå ´åˆ
            self.base_path = base_path
            if not self.base_path.startswith('/'):
                self.base_path = '/' + self.base_path
            if not self.base_path.endswith('/'):
                self.base_path = self.base_path + '/'
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿ï¼‰
            # ä¾‹: /run/docs/fit-for-run â†’ /run/docs/
            path = parsed_url.path
            if path.endswith('/'):
                self.base_path = path
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤ã„ã¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿å–å¾—
                self.base_path = '/'.join(path.split('/')[:-1]) + '/'
                if not self.base_path.startswith('/'):
                    self.base_path = '/' + self.base_path
            
            # ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã«ã™ã‚‹
            if self.base_path == '//':
                self.base_path = '/'
            
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {self.base_path} (è‡ªå‹•åˆ¤å®š)")
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³: {self.base_domain}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–
        if self.use_javascript:
            self._setup_driver()
    
    def _setup_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            # ChromeDriverã‚’è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†
            service = Service(ChromeDriverManager().install())
            
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸï¼ˆJavaScriptå¯¾å¿œãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
        except Exception as e:
            logger.error(f"Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _close_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸŒ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
    
    def is_valid_url(self, url: str) -> bool:
        """
        æœ‰åŠ¹ãªURLã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            url: ãƒã‚§ãƒƒã‚¯å¯¾è±¡URL
            
        Returns:
            bool: æœ‰åŠ¹ãªå ´åˆTrue
        """
        parsed = urlparse(url)
        
        # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
        if parsed.netloc != self.base_domain:
            return False
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã‹ãƒã‚§ãƒƒã‚¯
        if not parsed.path.startswith(self.base_path):
            return False
        
        # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        excluded_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.doc', '.docx', '.xls', '.xlsx', '.mp4', '.mp3'}
        path_lower = parsed.path.lower()
        
        for ext in excluded_extensions:
            if path_lower.endswith(ext):
                return False
        
        # é™¤å¤–ãƒ‘ã‚¹
        excluded_paths = {'/admin/', '/api/', '/wp-admin/', '/login/', '/logout/'}
        for excluded_path in excluded_paths:
            if excluded_path in parsed.path:
                return False
        
        return True
    
    def extract_links(self, html_content: str, current_url: str) -> List[str]:
        """
        HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            current_url: ç¾åœ¨ã®ãƒšãƒ¼ã‚¸URL
            
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(current_url, href)
            
            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#sectionï¼‰ã‚’é™¤å»
            parsed = urlparse(absolute_url)
            clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
            
            if self.is_valid_url(clean_url) and clean_url not in self.visited_urls:
                links.append(clean_url)
        
        return links
    
    def extract_text_content(self, html_content: str, url: str) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ã®é™çš„ç‰ˆï¼‰
        
        Args:
            html_content: HTMLæ–‡å­—åˆ—
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        # main, article, div.content ãªã©ã®è¦ç´ ã‚’å„ªå…ˆçš„ã«æ¢ã™
        main_content = None
        for selector in ['main', 'article', '.content', '.main-content', '#content', '#main']:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
        if not main_content:
            main_content = soup.find('body')
        
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
        
        return formatted_content
    
    def extract_text_content_js(self, url: str) -> str:
        """
        Seleniumã§JavaScriptå®Ÿè¡Œå¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        
        Args:
            url: ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            logger.info(f"ğŸŒ JavaScriptå®Ÿè¡Œä¸­: {url}")
            self.driver.get(url)
            
            # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†ã®ãŸã‚ï¼‰
            time.sleep(3)
            
            # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            page_source = self.driver.page_source
            
            # BeautifulSoupã§è§£æ
            soup = BeautifulSoup(page_source, 'lxml')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            main_content = None
            for selector in ['main', 'article', '.content', '.main-content', '#content', '#main', '[role="main"]']:
                try:
                    if selector.startswith('.') or selector.startswith('#') or selector.startswith('['):
                        main_content = soup.select_one(selector)
                    else:
                        main_content = soup.find(selector)
                    if main_content:
                        break
                except:
                    continue
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyã‚’ä½¿ç”¨
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
            
            return formatted_content
            
        except TimeoutException:
            logger.error(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n{'='*50}\n\n"
        except WebDriverException as e:
            logger.error(f"WebDriverã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return f"\n{'='*50}\nURL: {url}\nã‚¨ãƒ©ãƒ¼: {e}\n{'='*50}\n\n"
    
    def scrape_page(self, url: str) -> bool:
        """
        å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
            
        Returns:
            bool: æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            if self.use_javascript:
                # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                content = self.extract_text_content_js(url)
                self.extracted_content.append(content)
                
                # ãƒªãƒ³ã‚¯æŠ½å‡ºã‚‚Seleniumã§å®Ÿè¡Œ
                page_source = self.driver.page_source
                new_links = self.extract_links(page_source, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
                
            else:
                # å¾“æ¥ã®é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                logger.info(f"å–å¾—ä¸­: {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                content = self.extract_text_content(response.text, url)
                self.extracted_content.append(content)
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(response.text, url)
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit_urls:
                        self.to_visit_urls.append(link)
            
            return True
            
        except requests.RequestException as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            return False
    
    def discover_all_pages(self) -> List[str]:
        """
        å…¨ãƒšãƒ¼ã‚¸ã®URLã‚’äº‹å‰ã«æ¢ç´¢ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            List[str]: ç™ºè¦‹ã•ã‚ŒãŸå…¨ãƒšãƒ¼ã‚¸URLã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ğŸ“‹ äº‹å‰æ¢ç´¢é–‹å§‹: å…¨ãƒšãƒ¼ã‚¸URLã‚’åé›†ä¸­...")
        
        discovered_urls: Set[str] = set()
        to_explore: List[str] = [self.base_url]
        explored: Set[str] = set()
        
        while to_explore:
            current_url = to_explore.pop(0)
            
            if current_url in explored:
                continue
                
            explored.add(current_url)
            discovered_urls.add(current_url)
            
            try:
                logger.info(f"æ¢ç´¢ä¸­: {current_url}")
                
                if self.use_javascript:
                    # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
                    self.driver.get(current_url)
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    page_source = self.driver.page_source
                else:
                    # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    response = self.session.get(current_url, timeout=30)
                    response.raise_for_status()
                    page_source = response.text
                
                # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’åé›†
                new_links = self.extract_links(page_source, current_url)
                for link in new_links:
                    if link not in discovered_urls and link not in to_explore:
                        to_explore.append(link)
                
                # é€²æ—è¡¨ç¤ºï¼ˆ10ã®å€æ•°ã§è¡¨ç¤ºï¼‰
                if len(discovered_urls) % 10 == 0:
                    logger.info(f"ğŸ“Š ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {len(discovered_urls)}ãƒšãƒ¼ã‚¸")
                
            except Exception as e:
                logger.warning(f"æ¢ç´¢ã‚¨ãƒ©ãƒ¼ - {current_url}: {e}")
                continue
            
            # æ¢ç´¢é–“éš”ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚ˆã‚ŠçŸ­ã‚ã«ï¼‰
            time.sleep(self.delay * 0.5)
        
        sorted_urls = sorted(list(discovered_urls))
        logger.info(f"ğŸ“‹ äº‹å‰æ¢ç´¢å®Œäº†: ç·ãƒšãƒ¼ã‚¸æ•° {len(sorted_urls)}ãƒšãƒ¼ã‚¸")
        
        return sorted_urls
    
    def scrape_website(self) -> (int, int):
        """
        Webã‚µã‚¤ãƒˆå…¨ä½“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆJavaScriptå¯¾å¿œç‰ˆï¼‰
        
        Returns:
            (int, int): ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°, å–å¾—ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {self.base_url}")
            logger.info(f"å¯¾è±¡ç¯„å›²: {self.base_domain}{self.base_path}* é…ä¸‹")
            if self.use_javascript:
                logger.info("ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚å¯¾å¿œ")
            
            # äº‹å‰æ¢ç´¢ã§å…¨ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹
            all_pages = self.discover_all_pages()
            total_pages = len(all_pages)
            
            print(f"\nğŸ” äº‹å‰æ¢ç´¢çµæœ:")
            print(f"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸")
            
            if self.max_pages is None:
                pages_to_process = total_pages
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™ãªã—)")
            else:
                pages_to_process = min(total_pages, self.max_pages)
                print(f"ğŸ“¥ å‡¦ç†äºˆå®šãƒšãƒ¼ã‚¸æ•°: {pages_to_process}ãƒšãƒ¼ã‚¸ (åˆ¶é™: {self.max_pages})")
                if total_pages > self.max_pages:
                    print(f"âš ï¸  æ³¨æ„: {total_pages - self.max_pages}ãƒšãƒ¼ã‚¸ãŒåˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–ã•ã‚Œã¾ã™")
            
            print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: ç´„{pages_to_process * self.delay / 60:.1f}åˆ†")
            print("-" * 60)
            
            # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
            logger.info(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹...")
            
            # å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’è¨­å®š
            self.to_visit_urls = all_pages[:pages_to_process] if self.max_pages else all_pages
            self.visited_urls = set()  # ãƒªã‚»ãƒƒãƒˆ
            
            page_count = 0
            
            while self.to_visit_urls:
                current_url = self.to_visit_urls.pop(0)
                
                if current_url in self.visited_urls:
                    continue
                
                self.visited_urls.add(current_url)
                
                if self.scrape_page(current_url):
                    page_count += 1
                    if self.max_pages is None:
                        logger.info(f"é€²æ—: {page_count}/{total_pages}")
                    else:
                        max_display = min(total_pages, self.max_pages)
                        logger.info(f"é€²æ—: {page_count}/{max_display}")
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
                time.sleep(self.delay)
            
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {page_count}ãƒšãƒ¼ã‚¸å–å¾—")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
            domain_name = self.base_domain.replace('.', '_')
            path_name = self.base_path.replace('/', '_').strip('_')
            if path_name:
                filename_base = f"{domain_name}_{path_name}"
            else:
                filename_base = domain_name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"{filename_base}_all_content_{timestamp}.txt"
            
            # åˆ†å‰²ä¿å­˜ã‚’å®Ÿè¡Œ
            self.save_content_split(base_filename, total_pages, page_count)
            
            return total_pages, page_count
            
        finally:
            # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.use_javascript:
                self._close_driver()
    
    def save_content_split(self, base_filename: str, total_pages: int, pages_processed: int):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜
        
        Args:
            base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
            total_pages: ç™ºè¦‹ã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°
            pages_processed: å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°
        """
        if not self.extracted_content:
            logger.warning("ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åˆ†å‰²æ•°ã‚’è¨ˆç®—
        total_files = (len(self.extracted_content) + self.pages_per_file - 1) // self.pages_per_file
        
        logger.info(f"ğŸ“‚ åˆ†å‰²ä¿å­˜é–‹å§‹: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¾ã™")
        print(f"\nğŸ“‚ åˆ†å‰²ä¿å­˜ä¸­...")
        print(f"ğŸ—‚ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²: {self.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ã€è¨ˆ{total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ†é›¢
        base_name = base_filename.replace('.txt', '')
        
        saved_files = []
        
        for file_index in range(total_files):
            start_idx = file_index * self.pages_per_file
            end_idx = min(start_idx + self.pages_per_file, len(self.extracted_content))
            
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            file_content = self.extracted_content[start_idx:end_idx]
            pages_in_file = len(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            filename = f"{base_name}_part{file_index + 1}_of_{total_files}.txt"
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
            domain_name = self.base_domain
            header = f"""NotebookLMç”¨ å…¨ã‚µã‚¤ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ãƒ‘ãƒ¼ãƒˆ {file_index + 1}/{total_files})
ã‚µã‚¤ãƒˆ: {domain_name}
å¯¾è±¡ãƒ‘ã‚¹: {self.base_path}* é…ä¸‹
æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸
å–å¾—ãƒšãƒ¼ã‚¸æ•°: {pages_processed}ãƒšãƒ¼ã‚¸
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: {pages_in_file}ãƒšãƒ¼ã‚¸ (ãƒšãƒ¼ã‚¸{start_idx + 1}ã€œ{end_idx})

{'='*80}

"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµåˆ
            full_content = header + "\n".join(file_content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(full_content)
                
                file_size_kb = len(full_content.encode('utf-8')) / 1024
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({file_size_kb:.1f} KB)")
                saved_files.append(filename)
                
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {filename}: {e}")
        
        # ä¿å­˜çµæœã®è¡¨ç¤º
        print(f"\nâœ… åˆ†å‰²ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}å€‹")
        for i, filename in enumerate(saved_files, 1):
            file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
            print(f"   {i}. {filename} ({file_size_kb:.1f} KB)")
        
        print(f"\nğŸ’¡ NotebookLMã§ã®ä½¿ç”¨æ–¹æ³•:")
        print(f"   å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        print(f"   ãƒ‘ãƒ¼ãƒˆ1ã‹ã‚‰é †ç•ªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™")
    
    def save_content(self, content: str, filename: str):
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå¾“æ¥ã®å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼‰
        
        Args:
            content: ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")
            print(f"\nâœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content.encode('utf-8')) / 1024:.1f} KB")
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def process_multiple_urls(url_list: List[str], max_pages: int = None, delay: float = 1.0, 
                         base_path: str = None, pages_per_file: int = 80, use_javascript: bool = False, 
                         exact_urls: bool = False):
    """
    è¤‡æ•°URLã‚’é †æ¬¡å‡¦ç†ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ
    
    Args:
        url_list: å‡¦ç†å¯¾è±¡URLã®ãƒªã‚¹ãƒˆ
        max_pages: 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šã®æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆexact_urlsãŒTrueã®å ´åˆã¯ç„¡è¦–ï¼‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆexact_urlsãŒTrueã®å ´åˆã¯ç„¡è¦–ï¼‰
        pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        exact_urls: Trueã®å ´åˆã€æŒ‡å®šã•ã‚ŒãŸURLã®ã¿ã‚’å‡¦ç†ï¼ˆãƒªãƒ³ã‚¯è¿½è·¡ãªã—ï¼‰
        
    Returns:
        (int, int): ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°ã€ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°
    """
    all_content = []
    total_discovered = 0
    total_processed = 0
    
    if exact_urls:
        print(f"\nğŸ¯ æŒ‡å®šURLé™å®šå‡¦ç†é–‹å§‹: {len(url_list)}ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥å‡¦ç†")
        print("ğŸ“Œ å„URLã®ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—ï¼ˆä¸‹ä½ãƒšãƒ¼ã‚¸ã®è‡ªå‹•åé›†ã¯è¡Œã„ã¾ã›ã‚“ï¼‰")
    else:
        print(f"\nğŸ”— è¤‡æ•°URLå‡¦ç†é–‹å§‹: {len(url_list)}ã‚µã‚¤ãƒˆã‚’é †æ¬¡å‡¦ç†")
        print("ğŸŒ å„URLã‚’èµ·ç‚¹ã¨ã—ã¦ä¸‹ä½ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•åé›†ã—ã¾ã™")
    print("-" * 60)
    
    for i, url in enumerate(url_list, 1):
        print(f"\nğŸ“ [{i}/{len(url_list)}] å‡¦ç†ä¸­: {url}")
        print("-" * 40)
        
        try:
            if exact_urls:
                # æŒ‡å®šURLã®ã¿å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                content = scrape_single_url(url, use_javascript, delay)
                if content:
                    all_content.append(content)
                    total_processed += 1
                    total_discovered += 1
                    print(f"âœ… [{i}/{len(url_list)}] å®Œäº†: 1ãƒšãƒ¼ã‚¸å–å¾—")
                else:
                    print(f"âŒ [{i}/{len(url_list)}] å¤±æ•—: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            else:
                # å¾“æ¥ã®ä¸‹ä½ãƒšãƒ¼ã‚¸è‡ªå‹•åé›†ãƒ¢ãƒ¼ãƒ‰
                scraper = WebsiteScraper(url, max_pages, delay, base_path, pages_per_file, use_javascript)
                discovered, processed = scraper.scrape_website()
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ
                all_content.extend(scraper.extracted_content)
                total_discovered += discovered
                total_processed += processed
                
                print(f"âœ… [{i}/{len(url_list)}] å®Œäº†: {processed}ãƒšãƒ¼ã‚¸å–å¾—")
            
            # ã‚µã‚¤ãƒˆé–“ã®é–“éš”
            if i < len(url_list):
                time.sleep(delay * 2)  # ã‚µã‚¤ãƒˆé–“ã¯é€šå¸¸ã®2å€ã®é–“éš”
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
            print(f"âŒ [{i}/{len(url_list)}] ã‚¹ã‚­ãƒƒãƒ—: {url} - {e}")
            continue
    
    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    if all_content:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if exact_urls:
            base_filename = f"exact_urls_content_{timestamp}.txt"
        else:
            base_filename = f"multi_site_content_{timestamp}.txt"
        save_content_split_unified(all_content, base_filename, total_discovered, total_processed, pages_per_file)
    
    return total_discovered, total_processed


def scrape_single_url(url: str, use_javascript: bool = False, delay: float = 1.0) -> str:
    """
    å˜ä¸€URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’å–å¾—ï¼ˆãƒªãƒ³ã‚¯è¿½è·¡ãªã—ï¼‰
    
    Args:
        url: å–å¾—å¯¾è±¡URL
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: é…å»¶æ™‚é–“ï¼ˆJavaScriptå®Ÿè¡Œæ™‚ã®å¾…æ©Ÿã«ä½¿ç”¨ï¼‰
        
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯Noneï¼‰
    """
    try:
        if use_javascript:
            # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            logger.info(f"ğŸŒ JavaScriptå®Ÿè¡Œä¸­: {url}")
            
            # ç°¡æ˜“çš„ãªSeleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.set_page_load_timeout(30)
            
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                
                page_source = driver.page_source
                
                # BeautifulSoupã§è§£æ
                soup = BeautifulSoup(page_source, 'lxml')
                
                # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
                for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    element.decompose()
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title = soup.find('title')
                title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                main_content = None
                for selector in ['main', 'article', '.content', '.main-content', '#content', '#main', '[role="main"]']:
                    try:
                        if selector.startswith('.') or selector.startswith('#') or selector.startswith('['):
                            main_content = soup.select_one(selector)
                        else:
                            main_content = soup.find(selector)
                        if main_content:
                            break
                    except:
                        continue
                
                if not main_content:
                    main_content = soup.find('body')
                
                if main_content:
                    text = main_content.get_text(separator='\n', strip=True)
                else:
                    text = soup.get_text(separator='\n', strip=True)
                
                # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
                text = re.sub(r'\n\s*\n', '\n\n', text)
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
                
                return formatted_content
                
            finally:
                driver.quit()
                
        else:
            # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            logger.info(f"å–å¾—ä¸­: {url}")
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            main_content = None
            for selector in ['main', 'article', '.content', '.main-content', '#content', '#main']:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_content = f"\n{'='*50}\nURL: {url}\nã‚¿ã‚¤ãƒˆãƒ«: {title_text}\n{'='*50}\n\n{text}\n\n"
            
            return formatted_content
            
    except Exception as e:
        logger.error(f"å˜ä¸€URLå–å¾—ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
        return None


def save_content_split_unified(content_list: List[str], base_filename: str, 
                              total_discovered: int, total_processed: int, pages_per_file: int):
    """
    çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜
    
    Args:
        content_list: å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒªã‚¹ãƒˆ
        base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
        total_discovered: ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°
        total_processed: ç·å‡¦ç†ãƒšãƒ¼ã‚¸æ•°
        pages_per_file: 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°
    """
    if not content_list:
        logger.warning("ä¿å­˜ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # åˆ†å‰²æ•°ã‚’è¨ˆç®—
    total_files = (len(content_list) + pages_per_file - 1) // pages_per_file
    
    logger.info(f"ğŸ“‚ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¾ã™")
    print(f"\nğŸ“‚ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­...")
    print(f"ğŸ—‚ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²: {pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ã€è¨ˆ{total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ†é›¢
    base_name = base_filename.replace('.txt', '')
    
    saved_files = []
    
    for file_index in range(total_files):
        start_idx = file_index * pages_per_file
        end_idx = min(start_idx + pages_per_file, len(content_list))
        
        # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        file_content = content_list[start_idx:end_idx]
        pages_in_file = len(file_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        filename = f"{base_name}_part{file_index + 1}_of_{total_files}.txt"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
        header = f"""NotebookLMç”¨ è¤‡æ•°ã‚µã‚¤ãƒˆçµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ãƒ‘ãƒ¼ãƒˆ {file_index + 1}/{total_files})
æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_discovered}ãƒšãƒ¼ã‚¸
ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: {pages_in_file}ãƒšãƒ¼ã‚¸ (ãƒšãƒ¼ã‚¸{start_idx + 1}ã€œ{end_idx})

{'='*80}

"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµåˆ
        full_content = header + "\n".join(file_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(full_content)
            
            file_size_kb = len(full_content.encode('utf-8')) / 1024
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({file_size_kb:.1f} KB)")
            saved_files.append(filename)
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {filename}: {e}")
    
    # ä¿å­˜çµæœã®è¡¨ç¤º
    print(f"\nâœ… çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†!")
    print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}å€‹")
    for i, filename in enumerate(saved_files, 1):
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        print(f"   {i}. {filename} ({file_size_kb:.1f} KB)")
    
    print(f"\nğŸ’¡ NotebookLMã§ã®ä½¿ç”¨æ–¹æ³•:")
    print(f"   å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    print(f"   ãƒ‘ãƒ¼ãƒˆ1ã‹ã‚‰é †ç•ªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™")


def extract_page_metadata(url: str, use_javascript: bool = False) -> dict:
    """
    å˜ä¸€ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆURLã€titleã€h1ï¼‰ã‚’æŠ½å‡º
    
    Args:
        url: å–å¾—å¯¾è±¡URL
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        
    Returns:
        dict: {'url': str, 'title': str, 'h1': str, 'status': str}
    """
    try:
        if use_javascript:
            # JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.set_page_load_timeout(30)
            
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†å¾…æ©Ÿ
                
                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'lxml')
                
            finally:
                driver.quit()
        else:
            # é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
        
        # titleã‚¿ã‚°ã‚’å–å¾—
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else ""
        
        # h1ã‚¿ã‚°ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®ã‚‚ã®ï¼‰
        h1_tag = soup.find('h1')
        h1 = h1_tag.get_text().strip() if h1_tag else ""
        
        return {
            'url': url,
            'title': title,
            'h1': h1,
            'status': 'success'
        }
        
    except Exception as e:
        logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ - {url}: {e}")
        return {
            'url': url,
            'title': "",
            'h1': "",
            'status': f'error: {str(e)}'
        }


def generate_sitemap(base_url: str, base_path: str = None, use_javascript: bool = False, 
                    delay: float = 1.0, output_format: str = 'csv') -> str:
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼ˆURLã€titleã€h1ã®ãƒªã‚¹ãƒˆï¼‰ã‚’ç”Ÿæˆ
    
    Args:
        base_url: åŸºæº–URL
        base_path: ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        use_javascript: JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        output_format: å‡ºåŠ›å½¢å¼ï¼ˆ'csv' ã¾ãŸã¯ 'txt'ï¼‰
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    logger.info(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆé–‹å§‹: {base_url}")
    
    # WebsiteScraperã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦å…¨URLã‚’ç™ºè¦‹
    scraper = WebsiteScraper(base_url, max_pages=None, delay=delay, 
                           base_path=base_path, use_javascript=use_javascript)
    
    print(f"\nğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆé–‹å§‹")
    print(f"ğŸ“ å¯¾è±¡URL: {base_url}")
    print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {scraper.base_path}")
    if use_javascript:
        print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹")
    else:
        print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
    print(f"â±ï¸  é…å»¶æ™‚é–“: {delay}ç§’")
    print("-" * 50)
    
    # å…¨ãƒšãƒ¼ã‚¸URLã‚’ç™ºè¦‹
    all_urls = scraper.discover_all_pages()
    
    print(f"\nğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹: {len(all_urls)}ãƒšãƒ¼ã‚¸")
    print("-" * 40)
    
    # å„URLã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    metadata_list = []
    
    for i, url in enumerate(all_urls, 1):
        print(f"ğŸ” [{i}/{len(all_urls)}] å‡¦ç†ä¸­: {url}")
        
        metadata = extract_page_metadata(url, use_javascript)
        metadata_list.append(metadata)
        
        # é€²æ—è¡¨ç¤º
        if metadata['status'] == 'success':
            title_preview = metadata['title'][:50] + "..." if len(metadata['title']) > 50 else metadata['title']
            print(f"   âœ… Title: {title_preview}")
        else:
            print(f"   âŒ {metadata['status']}")
        
        # é…å»¶
        if i < len(all_urls):
            time.sleep(delay)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    domain_name = scraper.base_domain.replace('.', '_')
    path_name = scraper.base_path.replace('/', '_').strip('_')
    
    if path_name:
        filename_base = f"{domain_name}_{path_name}_sitemap_{timestamp}"
    else:
        filename_base = f"{domain_name}_sitemap_{timestamp}"
    
    if output_format.lower() == 'csv':
        filename = f"{filename_base}.csv"
        save_sitemap_csv(metadata_list, filename)
    else:
        filename = f"{filename_base}.txt"
        save_sitemap_txt(metadata_list, filename)
    
    # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if use_javascript and scraper.driver:
        scraper._close_driver()
    
    return filename


def save_sitemap_csv(metadata_list: list, filename: str):
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’CSVå½¢å¼ã§ä¿å­˜
    
    Args:
        metadata_list: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    try:
        with open(filename, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            writer.writerow(['URL', 'Title', 'H1', 'Status'])
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for metadata in metadata_list:
                writer.writerow([
                    metadata['url'],
                    metadata['title'],
                    metadata['h1'],
                    metadata['status']
                ])
        
        success_count = sum(1 for m in metadata_list if m['status'] == 'success')
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        
        print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_kb:.1f} KB")
        print(f"ğŸ“Š ç·URLæ•°: {len(metadata_list)}")
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {len(metadata_list) - success_count}")
        
        logger.info(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—CSVä¿å­˜å®Œäº†: {filename}")
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def save_sitemap_txt(metadata_list: list, filename: str):
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’TXTå½¢å¼ã§ä¿å­˜
    
    Args:
        metadata_list: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            f.write("ã‚µã‚¤ãƒˆãƒãƒƒãƒ— - URLãƒ»Titleãƒ»H1 ä¸€è¦§\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç·URLæ•°: {len(metadata_list)}\n")
            f.write("=" * 80 + "\n\n")
            
            # ãƒ‡ãƒ¼ã‚¿
            for i, metadata in enumerate(metadata_list, 1):
                f.write(f"{i}. URL: {metadata['url']}\n")
                f.write(f"   Title: {metadata['title']}\n")
                f.write(f"   H1: {metadata['h1']}\n")
                f.write(f"   Status: {metadata['status']}\n")
                f.write("-" * 40 + "\n")
        
        success_count = sum(1 for m in metadata_list if m['status'] == 'success')
        file_size_kb = len(open(filename, 'r', encoding='utf-8').read().encode('utf-8')) / 1024
        
        print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¿å­˜å®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_kb:.1f} KB")
        print(f"ğŸ“Š ç·URLæ•°: {len(metadata_list)}")
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {len(metadata_list) - success_count}")
        
        logger.info(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—TXTä¿å­˜å®Œäº†: {filename}")
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—TXTä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def main():
    parser = argparse.ArgumentParser(description='NotebookLMç”¨ Webã‚µã‚¤ãƒˆä¸€æ‹¬ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«')
    parser.add_argument('url', nargs='?', help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆURLï¼ˆ--url-listã¨æ’ä»–çš„ï¼‰')
    parser.add_argument('--url-list', type=str, help='URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰')
    parser.add_argument('--max-pages', type=int, default=1000, help='æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰')
    parser.add_argument('--delay', type=float, default=1.0, help='ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰')
    parser.add_argument('--base-path', type=str, default=None, help='ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆä¾‹: /run/docs/ï¼‰æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•åˆ¤å®š')
    parser.add_argument('--no-limit', action='store_true', help='ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆæ³¨æ„: å¤§é‡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰')
    parser.add_argument('--pages-per-file', type=int, default=80, help='1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 80ï¼‰NotebookLMã®åˆ¶é™ã«å¿œã˜ã¦èª¿æ•´')
    parser.add_argument('--javascript', action='store_true', help='JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œã€å‡¦ç†æ™‚é–“ãŒé•·ããªã‚Šã¾ã™ï¼‰')
    parser.add_argument('--exact-urls', action='store_true', help='æŒ‡å®šã•ã‚ŒãŸURLãƒªã‚¹ãƒˆã®URLã®ã¿ã‚’å‡¦ç†ã—ã€ãƒªãƒ³ã‚¯è¿½è·¡ã‚’è¡Œã‚ãªã„')
    parser.add_argument('--generate-sitemap', action='store_true', help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼ˆURLãƒ»titleãƒ»h1ã®ãƒªã‚¹ãƒˆï¼‰ã‚’ç”Ÿæˆã™ã‚‹')
    parser.add_argument('--sitemap-format', type=str, choices=['csv', 'txt'], default='csv', help='ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å‡ºåŠ›å½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: csvï¼‰')
    
    args = parser.parse_args()
    
    # URL ã¾ãŸã¯ URL-list ã®æ’ä»–çš„ãƒã‚§ãƒƒã‚¯
    if not args.url and not args.url_list:
        parser.error("URLã¾ãŸã¯--url-listã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    if args.url and args.url_list:
        parser.error("URLã¨--url-listã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    
    # --exact-urls ã¯ --url-list ã¨çµ„ã¿åˆã‚ã›ã¦ã®ã¿ä½¿ç”¨å¯èƒ½
    if args.exact_urls and not args.url_list:
        parser.error("--exact-urlsã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯--url-listã¨çµ„ã¿åˆã‚ã›ã¦ã®ã¿ä½¿ç”¨ã§ãã¾ã™")
    
    # --generate-sitemap ã¯å˜ä¸€URLã¨ã®ã¿çµ„ã¿åˆã‚ã›å¯èƒ½
    if args.generate_sitemap and args.url_list:
        parser.error("--generate-sitemapã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å˜ä¸€URLã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™")
    
    if args.generate_sitemap and args.exact_urls:
        parser.error("--generate-sitemapã¨--exact-urlsã¯åŒæ™‚ã«ä½¿ç”¨ã§ãã¾ã›ã‚“")
    
    # --no-limitãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ç„¡åˆ¶é™ã«
    max_pages = None if args.no_limit else args.max_pages
    
    if args.generate_sitemap:
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
        print(f"ğŸ“‹ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ å¯¾è±¡URL: {args.url}")
        if args.base_path:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: è‡ªå‹•åˆ¤å®š")
        print(f"ğŸ“Š å‡ºåŠ›å½¢å¼: {args.sitemap_format.upper()}")
        print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
        if args.javascript:
            print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
        else:
            print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
        print("-" * 50)
        
        try:
            filename = generate_sitemap(args.url, args.base_path, args.javascript, 
                                      args.delay, args.sitemap_format)
            print(f"\nğŸ‰ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆå®Œäº†ï¼")
            print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"ğŸ’¡ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚µã‚¤ãƒˆå…¨ä½“ã®æ§‹é€ ã‚’ç¢ºèªã§ãã¾ã™")
            
        except Exception as e:
            logger.error(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return
    
    elif args.url_list:
        # URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å‡¦ç†
        try:
            url_list = load_urls_from_file(args.url_list)
            
            print(f"ğŸš€ è¤‡æ•°Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            print(f"ğŸ“‹ URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {args.url_list}")
            print(f"ğŸ“Š å¯¾è±¡ã‚µã‚¤ãƒˆæ•°: {len(url_list)}ã‚µã‚¤ãƒˆ")
            if args.exact_urls:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: ç„¡è¦–ï¼ˆå„URLã‚’ç›´æ¥å‡¦ç†ï¼‰")
            elif args.base_path:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
            else:
                print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: å„ã‚µã‚¤ãƒˆã§è‡ªå‹•åˆ¤å®š")
            if args.exact_urls:
                print(f"ğŸ“Š ãƒšãƒ¼ã‚¸æ•°åˆ¶é™: å„URL1ãƒšãƒ¼ã‚¸ã®ã¿ï¼ˆ--max-pagesè¨­å®šã¯ç„¡è¦–ï¼‰")
            elif args.no_limit:
                print(f"ğŸ“Š 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™ âš ï¸")
            else:
                print(f"ğŸ“Š 1ã‚µã‚¤ãƒˆã‚ãŸã‚Šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
            print(f"ğŸ—‚ï¸  åˆ†å‰²è¨­å®š: {args.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²")
            print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
            if args.javascript:
                print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
            else:
                print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
            if args.exact_urls:
                print(f"ğŸ¯ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: æŒ‡å®šURLé™å®š (å„URLã®ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†)")
            else:
                print(f"ğŸŒ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ã‚µã‚¤ãƒˆå…¨ä½“åé›† (å„URLã‹ã‚‰ä¸‹ä½ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•åé›†)")
            print("-" * 50)
            
            total_discovered, total_processed = process_multiple_urls(
                url_list, max_pages, args.delay, args.base_path, args.pages_per_file, args.javascript, args.exact_urls
            )
            
            if args.exact_urls:
                print("\nğŸ‰ æŒ‡å®šURLé™å®šå‡¦ç†å®Œäº†ï¼")
                print(f"ğŸ“¥ å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸")
                print(f"NotebookLMã«æŒ‡å®šã•ã‚ŒãŸ{total_processed}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")
            else:
                print("\nğŸ‰ è¤‡æ•°ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
                print(f"ğŸ“Š ç·ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {total_discovered}ãƒšãƒ¼ã‚¸")
                print(f"ğŸ“¥ ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°: {total_processed}ãƒšãƒ¼ã‚¸")
                print(f"NotebookLMã«çµ±åˆã•ã‚ŒãŸ{total_processed}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")
            
        except Exception as e:
            logger.error(f"URLãƒªã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return
    
    else:
        # å˜ä¸€URLã‹ã‚‰ã®å‡¦ç†ï¼ˆæ—¢å­˜ã®å‡¦ç†ï¼‰
        print(f"ğŸš€ Webã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        print(f"ğŸ“ å¯¾è±¡URL: {args.url}")
        if args.base_path:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.base_path} (æ‰‹å‹•æŒ‡å®š)")
        else:
            print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: è‡ªå‹•åˆ¤å®š")
        if args.no_limit:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: ç„¡åˆ¶é™ âš ï¸")
        else:
            print(f"ğŸ“Š æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
        print(f"ğŸ—‚ï¸  åˆ†å‰²è¨­å®š: {args.pages_per_file}ãƒšãƒ¼ã‚¸ãšã¤ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²")
        print(f"â±ï¸  é…å»¶æ™‚é–“: {args.delay}ç§’")
        if args.javascript:
            print(f"ğŸ’» JavaScriptå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ (å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ)")
        else:
            print(f"ğŸŒ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: æ¨™æº–")
        print("-" * 50)
        
        scraper = WebsiteScraper(args.url, max_pages, args.delay, args.base_path, args.pages_per_file, args.javascript)
        total_pages, page_count = scraper.scrape_website()
        
        print("\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
        print(f"NotebookLMã« {page_count}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
